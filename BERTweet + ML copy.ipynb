{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"secrets.json\", \"r\") as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "\n",
    "from utils import load_tweeteval\n",
    "\n",
    "hateval = load_tweeteval()[\"hate\"]\n",
    "\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-cased were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ChrisZeng/bertweet-base-cased-covid19-hateval were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ChrisZeng/bertweet-base-cased-covid19-hateval and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google/t5-efficient-large were not used when initializing T5EncoderModel: ['decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.embed_tokens.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'lm_head.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.23.layer.2.layer_norm.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, RobertaModel, T5EncoderModel\n",
    "\n",
    "\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"vinai/bertweet-covid19-base-cased\", normalization=True\n",
    ")\n",
    "bertweet_model = RobertaModel.from_pretrained(\"vinai/bertweet-covid19-base-cased\")\n",
    "bertweet_ft_model = RobertaModel.from_pretrained(\n",
    "    \"ChrisZeng/bertweet-base-cased-covid19-hateval\"\n",
    ")\n",
    "bertweet_ft_classifier = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"ChrisZeng/bertweet-base-cased-covid19-hateval\"\n",
    ")\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "t5_model = T5EncoderModel.from_pretrained(\"google/t5-efficient-large\")\n",
    "\n",
    "transformer_models = {\n",
    "    \"bertweet\": {\"tokenizer\": bertweet_tokenizer, \"model\": bertweet_model},\n",
    "    \"bertweet-ft\": {\"tokenizer\": bertweet_tokenizer, \"model\": bertweet_ft_model},\n",
    "    \"t5\": {\"tokenizer\": t5_tokenizer, \"model\": t5_model},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc8b10cbc7242acbeb98656bc922957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967b4823954a4c3fa7444c0678611567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd69b237be649a0b52f21275e8cb141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b46d22ae2b2484bbe0b76f37f910b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b8ecb23dd447599fb8285ff6accfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9d21d430f042ac89781793ac88e4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a8519541c048d8b430a1332d723cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aabf4c4e78d43b98cd79db14ff463be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b424a6f0a5984bb99bce041883a5dd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817c127267304a07ad7894e90523f1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b0138050a14fa795f42b2b8c6dca8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c1960e34f84da381b4fea74922cd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import indice2logits\n",
    "\n",
    "hateval = hateval.map(\n",
    "    lambda rec: indice2logits(rec[\"labels\"], 2), batched=True, batch_size=1024\n",
    ").rename_columns({\"labels\": \"label_categoricals\", \"label_logits\": \"labels\"})\n",
    "\n",
    "for model_name, model_group in transformer_models.items():\n",
    "    transformer_models[model_name][\"datasets\"] = hateval.map(\n",
    "        lambda rec: model_group[\"tokenizer\"](\n",
    "            rec[\"text\"],\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=8,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "        batch_size=None,\n",
    "    )\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/inference\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_accumulation_steps=128,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1622' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('train', 'accuracy', 0.846),\n",
       " ('train', 'f1', 0.8256603773584905),\n",
       " ('val', 'accuracy', 0.768),\n",
       " ('val', 'f1', 0.7588357588357587),\n",
       " ('test', 'accuracy', 0.5461279461279461),\n",
       " ('test', 'f1', 0.6401494927923118)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from utils import get_metrics, get_labels\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bertweet_ft_classifier, tokenizer=bertweet_tokenizer, args=training_args\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    results[\"baseline\"] = get_metrics(\n",
    "        lambda inputs: trainer.predict(inputs).predictions.argmax(axis=1),\n",
    "        transformer_models[\"bertweet-ft\"][\"datasets\"],\n",
    "        get_labels(transformer_models[\"bertweet-ft\"][\"datasets\"], \"label_categoricals\"),\n",
    "        [\"train\", \"val\", \"test\"],\n",
    "        {\"accuracy\": accuracy_score, \"f1\": f1_score},\n",
    "    )\n",
    "    \n",
    "del trainer\n",
    "results[\"baseline\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaModel.forward` and have been ignored: labels, text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1622' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaModel.forward` and have been ignored: labels, text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaModel.forward` and have been ignored: labels, text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaModel.forward` and have been ignored: labels, text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1622' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaModel.forward` and have been ignored: labels, text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaModel.forward` and have been ignored: labels, text, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `T5EncoderModel.forward` and have been ignored: token_type_ids, text, label_categoricals, labels.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1622' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 01:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `T5EncoderModel.forward` and have been ignored: token_type_ids, text, label_categoricals, labels.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `T5EncoderModel.forward` and have been ignored: token_type_ids, text, label_categoricals, labels.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_group in transformer_models.items():\n",
    "    trainer = Trainer(\n",
    "        model=model_group[\"model\"],\n",
    "        tokenizer=model_group[\"tokenizer\"],\n",
    "        args=training_args,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        transformer_models[model_name][\"embedded\"] = {\n",
    "            split: trainer.predict(model_group[\"datasets\"][split])\n",
    "            for split in [\"train\", \"val\", \"test\"]\n",
    "        }\n",
    "    del trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_early_stopping</th>\n",
       "      <th>param_l1_ratio</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.393124</td>\n",
       "      <td>0.034228</td>\n",
       "      <td>0.008598</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'l1_ra...</td>\n",
       "      <td>0.880912</td>\n",
       "      <td>0.854276</td>\n",
       "      <td>0.838372</td>\n",
       "      <td>0.893396</td>\n",
       "      <td>0.799903</td>\n",
       "      <td>0.853372</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.257539</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 10, 'early_stopping': True, 'l1_rati...</td>\n",
       "      <td>0.881870</td>\n",
       "      <td>0.858930</td>\n",
       "      <td>0.842171</td>\n",
       "      <td>0.892565</td>\n",
       "      <td>0.787172</td>\n",
       "      <td>0.852542</td>\n",
       "      <td>0.037104</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.310922</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'l1_ra...</td>\n",
       "      <td>0.879225</td>\n",
       "      <td>0.858328</td>\n",
       "      <td>0.840843</td>\n",
       "      <td>0.893759</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.852326</td>\n",
       "      <td>0.036217</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.310801</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>log</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 1, 'early_stopping': True, 'l1_ratio...</td>\n",
       "      <td>0.884483</td>\n",
       "      <td>0.860566</td>\n",
       "      <td>0.840323</td>\n",
       "      <td>0.889842</td>\n",
       "      <td>0.786032</td>\n",
       "      <td>0.852249</td>\n",
       "      <td>0.037545</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.299353</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.008579</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 1, 'early_stopping': True, 'l1_ratio...</td>\n",
       "      <td>0.882104</td>\n",
       "      <td>0.858011</td>\n",
       "      <td>0.842213</td>\n",
       "      <td>0.895589</td>\n",
       "      <td>0.781911</td>\n",
       "      <td>0.851965</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.377036</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 1000, 'early_stopping': True, 'l1_ra...</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.366866</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.310103</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.385124</td>\n",
       "      <td>0.017340</td>\n",
       "      <td>0.006502</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>0.4</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 100, 'early_stopping': True, 'l1_rat...</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.366866</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.310103</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.362145</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>0.2</td>\n",
       "      <td>log</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 100, 'early_stopping': True, 'l1_rat...</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.366866</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.310103</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.359460</td>\n",
       "      <td>0.012711</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 1000, 'early_stopping': True, 'l1_ra...</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.295940</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.374593</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>{'alpha': 10, 'early_stopping': True, 'l1_rati...</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.295940</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "152       0.393124      0.034228         0.008598        0.002386        0.01   \n",
       "275       0.257539      0.007765         0.006944        0.001529          10   \n",
       "145       0.310922      0.015028         0.004451        0.000740        0.01   \n",
       "221       0.310801      0.004445         0.006590        0.001342           1   \n",
       "220       0.299353      0.005921         0.008579        0.002567           1   \n",
       "..             ...           ...              ...             ...         ...   \n",
       "413       0.377036      0.010320         0.005610        0.001273        1000   \n",
       "353       0.385124      0.017340         0.006502        0.001600         100   \n",
       "341       0.362145      0.007225         0.005079        0.000868         100   \n",
       "427       0.359460      0.012711         0.005697        0.000932        1000   \n",
       "310       0.374593      0.003013         0.004728        0.000727          10   \n",
       "\n",
       "    param_early_stopping param_l1_ratio      param_loss param_max_iter  \\\n",
       "152                 True            0.8  modified_huber          10000   \n",
       "275                 True            0.0           hinge          10000   \n",
       "145                 True            0.7           hinge          10000   \n",
       "221                 True            0.0             log          10000   \n",
       "220                 True            0.0           hinge          10000   \n",
       "..                   ...            ...             ...            ...   \n",
       "413                 True            0.5   squared_hinge          10000   \n",
       "353                 True            0.4   squared_hinge          10000   \n",
       "341                 True            0.2             log          10000   \n",
       "427                 True            0.8  modified_huber          10000   \n",
       "310                 True            0.7           hinge          10000   \n",
       "\n",
       "    param_penalty                                             params  \\\n",
       "152    elasticnet  {'alpha': 0.01, 'early_stopping': True, 'l1_ra...   \n",
       "275    elasticnet  {'alpha': 10, 'early_stopping': True, 'l1_rati...   \n",
       "145    elasticnet  {'alpha': 0.01, 'early_stopping': True, 'l1_ra...   \n",
       "221    elasticnet  {'alpha': 1, 'early_stopping': True, 'l1_ratio...   \n",
       "220    elasticnet  {'alpha': 1, 'early_stopping': True, 'l1_ratio...   \n",
       "..            ...                                                ...   \n",
       "413    elasticnet  {'alpha': 1000, 'early_stopping': True, 'l1_ra...   \n",
       "353    elasticnet  {'alpha': 100, 'early_stopping': True, 'l1_rat...   \n",
       "341    elasticnet  {'alpha': 100, 'early_stopping': True, 'l1_rat...   \n",
       "427    elasticnet  {'alpha': 1000, 'early_stopping': True, 'l1_ra...   \n",
       "310    elasticnet  {'alpha': 10, 'early_stopping': True, 'l1_rati...   \n",
       "\n",
       "     split0_test_score  split1_test_score  split2_test_score  \\\n",
       "152           0.880912           0.854276           0.838372   \n",
       "275           0.881870           0.858930           0.842171   \n",
       "145           0.879225           0.858328           0.840843   \n",
       "221           0.884483           0.860566           0.840323   \n",
       "220           0.882104           0.858011           0.842213   \n",
       "..                 ...                ...                ...   \n",
       "413           0.296050           0.296050           0.366866   \n",
       "353           0.296050           0.366866           0.296050   \n",
       "341           0.296050           0.366866           0.296050   \n",
       "427           0.296050           0.296050           0.296050   \n",
       "310           0.296050           0.296050           0.296050   \n",
       "\n",
       "     split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "152           0.893396           0.799903         0.853372        0.033001   \n",
       "275           0.892565           0.787172         0.852542        0.037104   \n",
       "145           0.893759           0.789474         0.852326        0.036217   \n",
       "221           0.889842           0.786032         0.852249        0.037545   \n",
       "220           0.895589           0.781911         0.851965        0.039625   \n",
       "..                 ...                ...              ...             ...   \n",
       "413           0.295775           0.295775         0.310103        0.028382   \n",
       "353           0.295775           0.295775         0.310103        0.028382   \n",
       "341           0.295775           0.295775         0.310103        0.028382   \n",
       "427           0.295775           0.295775         0.295940        0.000135   \n",
       "310           0.295775           0.295775         0.295940        0.000135   \n",
       "\n",
       "     rank_test_score  \n",
       "152                1  \n",
       "275                2  \n",
       "145                3  \n",
       "221                4  \n",
       "220                5  \n",
       "..               ...  \n",
       "413              430  \n",
       "353              430  \n",
       "341              430  \n",
       "427              439  \n",
       "310              439  \n",
       "\n",
       "[440 rows x 19 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"loss\": [\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
    "    \"penalty\": [\"elasticnet\"],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"l1_ratio\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"max_iter\": [10000],\n",
    "    \"early_stopping\": [True],\n",
    "}\n",
    "\n",
    "tune = GridSearchCV(\n",
    "    SGDClassifier(), params, cv=5, scoring=\"f1_macro\", n_jobs=8, refit=True\n",
    ")\n",
    "tune.fit(embedded[\"train\"], labels[\"train\"])\n",
    "pd.DataFrame(tune.cv_results_).sort_values(\"mean_test_score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.532080892666011"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels[\"test\"], tune.predict(embedded[\"test\"]), average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GridSearchCV(cv=5, estimator=SGDClassifier(), n_jobs=8,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'early_stopping': [True],\n",
       "                         'l1_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n",
       "                                      0.8, 0.9, 1.0],\n",
       "                         'loss': ['hinge', 'log', 'modified_huber',\n",
       "                                  'squared_hinge', 'perceptron'],\n",
       "                         'max_iter': [10000], 'penalty': ['elasticnet']},\n",
       "             scoring='f1_macro')>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
