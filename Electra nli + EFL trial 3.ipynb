{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_token = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/electra-large-discriminator\", normalization=True\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\", num_labels=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataframe = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"./data/hateval/hateval2019_en_train.csv\"),\n",
    "        pd.read_csv(\"./data/hateval/hateval2019_en_dev.csv\"),\n",
    "        pd.read_csv(\"./data/hateval/hateval2019_en_test.csv\"),\n",
    "    ],\n",
    "    keys=[\"train\", \"validation\", \"test\"],\n",
    "    names=[\"split\", \"index\"],\n",
    ").reset_index()\n",
    "\n",
    "hypotheses = pd.Series(\n",
    "    [\n",
    "        \"This sentence contains offensive language toward women or immigrants.\",\n",
    "    ],\n",
    "    name=\"hypothesis\",\n",
    ")\n",
    "\n",
    "dataframe = dataframe.merge(hypotheses, how=\"cross\").rename(\n",
    "    columns={\"text\": \"premise\"})\n",
    "dataframe[\"label_categorical\"] = dataframe[\"HS\"] * (-2) + 2\n",
    "dataframe = dataframe[[\"split\", \"id\", \"premise\",\n",
    "                       \"hypothesis\", \"label_categorical\"]]\n",
    "\n",
    "datasets = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(dataframe[dataframe[\"split\"] == split])\n",
    "        for split in [\"train\", \"validation\", \"test\"]\n",
    "    }\n",
    ")\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e1b554e5c04ce290ece0607e0d016d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7086b2110ff4510873581d64d1c2736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d8f855ada642749f221217d5c9c2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11fbac5fcd647aa904823a1bed15d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdb8bc227504be89ec18d943f6ee2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b04a6624ee414fb2927bfbda73cb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def indice2logits(indice, num_classes):\n",
    "    indice = np.array(indice)\n",
    "    logits = np.zeros([len(indice), num_classes], dtype=float)\n",
    "    logits[np.arange(len(indice)), indice] = 1.0\n",
    "    return {\"label_logits\": logits}\n",
    "\n",
    "\n",
    "datasets = datasets.map(\n",
    "    lambda rec: tokenizer(\n",
    "        rec[\"premise\"],\n",
    "        rec[\"hypothesis\"],\n",
    "        padding=\"longest\",\n",
    "        max_length=512,\n",
    "        pad_to_multiple_of=8,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "datasets = datasets.map(\n",
    "    lambda rec: indice2logits(rec[\"label_categorical\"], 3),\n",
    "    batched=True,\n",
    "    batch_size=1024,\n",
    ")\n",
    "\n",
    "datasets = datasets.rename_column(\"label_logits\", \"labels\")\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, label_logits = eval_preds\n",
    "    preds = pred_logits.argmax(axis=1)\n",
    "    labels = label_logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running training *****\n",
      "  Num examples = 9000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 1400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1400 1:03:42, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.446569</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.712607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.753051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.363927</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.761691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.297300</td>\n",
       "      <td>0.339928</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.786256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.330239</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.801332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.320612</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.805045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.314624</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.808714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.314041</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.809987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.319587</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.797365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>0.321260</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.794507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.324402</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.794378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.307951</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.813988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.213900</td>\n",
       "      <td>0.322943</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.798038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.317229</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.803837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.313664</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.805661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.319028</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.802058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.203900</td>\n",
       "      <td>0.319431</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.803775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.319113</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.799750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.319934</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.801763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.197200</td>\n",
       "      <td>0.319676</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.802800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-70\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-70/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-70/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-70/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-70/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-140\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-140/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-140/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-140/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-140/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-210\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-210/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-210/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-210/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-210/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-280\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-280/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-280/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-280/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-280/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-350\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-350/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-350/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-420\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-420/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-420/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-420/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-420/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-490\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-490/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-490/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-490/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-490/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-560\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-560/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-560/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-560/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-560/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-630\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-630/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-630/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-630/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-630/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-700\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-700/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-700/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-770\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-770/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-770/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-770/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-770/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-840\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-840/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-840/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-840/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-840/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-910\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-910/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-910/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-910/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-910/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-980\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-980/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-980/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-980/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-980/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-1050\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-1050/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-1050/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-1050/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-1120\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-1120/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-1120/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-1120/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-1120/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-1190\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-1190/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-1190/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-1190/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-1190/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-1260\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-1260/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-1260/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-1260/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-1260/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-1330\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-1330/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-1330/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-1330/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-1330/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/electra-nli-efl/checkpoint-1400\n",
      "Configuration saved in outputs/electra-nli-efl/checkpoint-1400/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/checkpoint-1400/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/electra-nli-efl/checkpoint-840 (score: 0.3079507350921631).\n",
      "Saving model checkpoint to outputs/electra-nli-efl\n",
      "Configuration saved in outputs/electra-nli-efl/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/electra-nli-efl\",\n",
    "    overwrite_output_dir=True,\n",
    "    dataloader_num_workers=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=20,\n",
    "    logging_strategy=\"epoch\",\n",
    "    remove_unused_columns=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    eval_accumulation_steps=128,\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    learning_rate=1e-6,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    #push_to_hub=True,\n",
    "    #hub_strategy=\"all_checkpoints\",\n",
    "    #hub_model_id=\"ChrisZeng/electra-large-discriminator-nli-efl-hateval\",\n",
    "    #hub_token=hub_token,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_output = trainer.train(\n",
    "     #resume_from_checkpoint=True,\n",
    ")\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file outputs/electra-nli-efl/added_tokens.json. We won't load it.\n",
      "loading file outputs/electra-nli-efl/vocab.txt\n",
      "loading file outputs/electra-nli-efl/tokenizer.json\n",
      "loading file None\n",
      "loading file outputs/electra-nli-efl/special_tokens_map.json\n",
      "loading file outputs/electra-nli-efl/tokenizer_config.json\n",
      "loading configuration file outputs/electra-nli-efl/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"outputs/electra-nli-efl\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 1024,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file outputs/electra-nli-efl/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at outputs/electra-nli-efl.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: split, hypothesis, id, premise, __index_level_0__, label_categorical.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>0.816</td>\n",
       "      <td>0.813988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.558</td>\n",
       "      <td>0.526481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy        f1\n",
       "dev      0.816  0.813988\n",
       "test     0.558  0.526481"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"outputs/electra-nli-efl\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"outputs/electra-nli-efl\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/inference\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=True,\n",
    "    eval_accumulation_steps=128,\n",
    "    disable_tqdm=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args)\n",
    "\n",
    "def predict(trainer, dataset):\n",
    "    preds = trainer.predict(dataset).predictions.argmax(axis=1)\n",
    "    df = (\n",
    "        pd.DataFrame(\n",
    "            {\"id\": dataset[\"id\"], \"pred\": preds,\n",
    "                \"label\": dataset[\"label_categorical\"]}\n",
    "        )\n",
    "        .groupby(\"id\")\n",
    "        .mean()\n",
    "    )\n",
    "    df[\"pred\"] = (df[\"pred\"] > 1).astype(int) * 2\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "preds_test = predict(trainer, datasets[\"test\"])\n",
    "preds_dev = predict(trainer, datasets[\"validation\"])\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"dev\": {\n",
    "            \"accuracy\": accuracy_score(preds_dev[\"label\"], preds_dev[\"pred\"]),\n",
    "            \"f1\": f1_score(preds_dev[\"label\"], preds_dev[\"pred\"], average=\"macro\"),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_score(preds_test[\"label\"], preds_test[\"pred\"]),\n",
    "            \"f1\": f1_score(preds_test[\"label\"], preds_test[\"pred\"], average=\"macro\"),\n",
    "        },\n",
    "    }\n",
    ").transpose()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
