{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "\n",
    "dataset_dict = dataset.train_test_split(test_size=2 / 10, seed=42)\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"eval\": dataset_dict[\"test\"],\n",
    "        **dataset_dict[\"train\"].train_test_split(test_size=3 / 8, seed=42),\n",
    "    }\n",
    ")\n",
    "\n",
    "for split, dataset in dataset_dict.items():\n",
    "    dataset.to_pandas().to_csv(\n",
    "        data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\"), index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 8679\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3720\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"secrets.json\", \"r\") as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "splits = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(\n",
    "            pd.read_csv(\n",
    "                data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\")\n",
    "            )\n",
    "        )\n",
    "        for split in splits\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer, input_text, target_text):\n",
    "    encoding = tokenizer(input_text)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        encoding[\"labels\"] = tokenizer(target_text)[\"input_ids\"]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "def get_traning_args(model_name):\n",
    "    model_name = model_name[model_name.find(\"/\") + 1 :]\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=\"outputs/\" + model_name + \"-detox\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=20,\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=16,\n",
    "        eval_accumulation_steps=128,\n",
    "        dataloader_num_workers=3,\n",
    "        predict_with_generate=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        remove_unused_columns=True,\n",
    "        optim=\"adamw_apex_fused\",\n",
    "        bf16=True,\n",
    "        bf16_full_eval=True,\n",
    "        tf32=True,\n",
    "        gradient_checkpointing=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        push_to_hub=True,\n",
    "        hub_strategy=\"all_checkpoints\",\n",
    "        hub_model_id=model_name + \"-detox\",\n",
    "        hub_token=secrets[\"hub_token_write\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "  warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0f1054052f4931b62db715f6840d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8679 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5e31057aa04656835d6026d09d06ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa5e37849bc413cb37c9af4d1627120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3720 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris-zeng/csci544-project/outputs/bart-base-detox is already a clone of https://huggingface.co/ChrisZeng/bart-base-detox. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.add_tokens(\"<CSD>\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.update({\"use_cache\": False})\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "encoding = dataset_dict.map(\n",
    "    lambda rec: encode(tokenizer, rec[\"original\"], rec[\"censored\"]),\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=get_traning_args(model_name),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer, model=model, padding=\"longest\", pad_to_multiple_of=8\n",
    "    ),\n",
    "    train_dataset=encoding[\"train\"],\n",
    "    eval_dataset=encoding[\"eval\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from outputs/bart-base-detox/checkpoint-1350).\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running training *****\n",
      "  Num examples = 8679\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 2700\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 10\n",
      "  Continuing training from global step 1350\n",
      "  Will skip the first 10 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83000610706144dfa1397e39549be91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2700' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2700/2700 36:35, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.185777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.181999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.182661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.184280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.183421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>0.182027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.183651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.181359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>0.181473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>0.181861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-1485\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-1485/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-1485/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-1485/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-1485/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-1620\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-1620/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-1620/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-1620/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-1620/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-1755\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-1755/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-1755/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-1755/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-1755/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-1890\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-1890/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-1890/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-1890/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-1890/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2025\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2025/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2025/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2025/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2025/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2160\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2160/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2160/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2160/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2160/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2295\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2295/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2295/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2295/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2295/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2430\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2430/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2430/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2430/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2430/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2565\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2565/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2565/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2565/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2565/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2700\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2700/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2700/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2700/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2700/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/bart-base-detox/checkpoint-2430 (score: 0.1813586801290512).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2700, training_loss=0.08368111080593534, metrics={'train_runtime': 2197.9803, 'train_samples_per_second': 78.973, 'train_steps_per_second': 1.228, 'total_flos': 1.075972393746432e+16, 'train_loss': 0.08368111080593534, 'epoch': 20.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/bart-base-detox\n",
      "Configuration saved in outputs/bart-base-detox/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "Saving model checkpoint to outputs/bart-base-detox\n",
      "Configuration saved in outputs/bart-base-detox/config.json\n",
      "Model weights saved in outputs/bart-base-detox/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2cad4e31544d60ab30bf5c126ad583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/optimizer.pt:   0%|          | 32.0k/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f874399bf04840ed8364bd6c5a423e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/scheduler.pt: 100%|##########| 623/623 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db12acfb411a4bdbb0a3283467190ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/pytorch_model.bin:   0%|          | 32.0k/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6d8466e9ef4e5a94928144da05742e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/scaler.pt: 100%|##########| 559/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f7be5ee4944ad192b23053fa6d3faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/rng_state.pth: 100%|##########| 14.2k/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/ChrisZeng/bart-base-detox\n",
      "   22aac39..c9e4d58  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "To https://huggingface.co/ChrisZeng/bart-base-detox\n",
      "   c9e4d58..c2af807  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/3d051d400035f2ca53580da38002eb0d2b7e188715710019c196e53ce2863c9c.4216d65d19bd74d36444d35fc3c0231b7f18539c9abaa91f81207dcadbf71eb9\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"ChrisZeng/t5-v1_1-base-detox\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32101\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/4c20b41026a8477b1db11f1a60fa8b54001dea569cf40a81a09c9cf955cef921.e1368003b4dd9328d37136550d0f086f2b1ee2d5691ac5494ed10f93a3ea8c6e\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ChrisZeng/t5-v1_1-base-detox.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/spiece.model from cache at /home/chris-zeng/.cache/huggingface/transformers/fd81b59c49c7368bd8b067b271985d624fc0ff3a2031041d93c44e1ad1a0d57b.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/tokenizer.json from cache at /home/chris-zeng/.cache/huggingface/transformers/5b45c8a95d30db679b4dc156d04ca211868050290b548c9dfd4e8de6e313f4a4.7548eb378c431c6b8417c829064c28670d7d1f35cd6fa88bb5de9da11fbe9dac\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/added_tokens.json from cache at /home/chris-zeng/.cache/huggingface/transformers/42c42f5e1b33129c0789a892f9313d0928e65a7d4157ba33fc1a42464975e044.e6cf36755a4a23d7c5dc882dc268bb487808b6233a7cde8384de4c94b0bbd102\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/special_tokens_map.json from cache at /home/chris-zeng/.cache/huggingface/transformers/d435947d720628841707d02794c46edbaf8cf7adb12203d40e24908eb93c3428.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/tokenizer_config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/8ca641d3ceb800bd8093b1e6e7dde17c6bacfd70698429d3810bc68b514f8af0.e4f38c0fb7cab0ed96b2b6e2b88a3d32d8fe5b2cc267d0dd8816db923e309305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc86d0be3ea94fadb56b2933af44cc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f34ce17ca34448bc1b09f7ea07d7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366c88687d5d43d79ff12ff37b97eaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def detox(tokenizer, model_buffered, batched_inputs):\n",
    "    input_sequence_buffered = tokenizer(\n",
    "        batched_inputs, padding=\"longest\", pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_sequence = model_buffered.generate(\n",
    "            input_ids=input_sequence_buffered[\"input_ids\"],\n",
    "            attention_mask=input_sequence_buffered[\"attention_mask\"],\n",
    "        )\n",
    "    del input_sequence_buffered\n",
    "    generated = [\n",
    "        \"<CSD>\" if len(generated) == 0 else generated\n",
    "        for generated in tokenizer.batch_decode(\n",
    "            output_sequence, skip_special_tokens=True\n",
    "        )\n",
    "    ]\n",
    "    return {\"generated\": generated}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "splits = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(\n",
    "            pd.read_csv(\n",
    "                data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\")\n",
    "            )\n",
    "        )\n",
    "        for split in splits\n",
    "    }\n",
    ")\n",
    "\n",
    "model_name = \"ChrisZeng/t5-v1_1-base-detox\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_buffered = model.to(\"cuda\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset_dict = dataset_dict.map(\n",
    "    lambda rec: detox(tokenizer, model_buffered, rec[\"original\"]),\n",
    "    keep_in_memory=True,\n",
    "    batched=True,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "del model_buffered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /home/chris-zeng/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /home/chris-zeng/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /home/chris-zeng/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>exact_match_rate</th>\n",
       "      <th>mean_bertscore_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.574129</td>\n",
       "      <td>0.498884</td>\n",
       "      <td>0.569738</td>\n",
       "      <td>0.569856</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.909635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval</th>\n",
       "      <td>0.584921</td>\n",
       "      <td>0.506518</td>\n",
       "      <td>0.580766</td>\n",
       "      <td>0.581342</td>\n",
       "      <td>0.080323</td>\n",
       "      <td>0.910855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.575102</td>\n",
       "      <td>0.496492</td>\n",
       "      <td>0.570006</td>\n",
       "      <td>0.570352</td>\n",
       "      <td>0.079839</td>\n",
       "      <td>0.909516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rouge1    rouge2    rougeL  rougeLsum  exact_match_rate  \\\n",
       "train  0.574129  0.498884  0.569738   0.569856          0.091600   \n",
       "eval   0.584921  0.506518  0.580766   0.581342          0.080323   \n",
       "test   0.575102  0.496492  0.570006   0.570352          0.079839   \n",
       "\n",
       "       mean_bertscore_f1  \n",
       "train           0.909635  \n",
       "eval            0.910855  \n",
       "test            0.909516  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Pretty\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "exact_match = load_metric(\"exact_match\")\n",
    "bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, targets):\n",
    "    return {\n",
    "        **{\n",
    "            key: value.mid.fmeasure\n",
    "            for key, value in rouge.compute(\n",
    "                predictions=predictions, references=targets\n",
    "            ).items()\n",
    "        },\n",
    "        \"exact_match_rate\": 0.01\n",
    "        * exact_match.compute(predictions=predictions, references=targets)[\n",
    "            \"exact_match\"\n",
    "        ],\n",
    "        \"mean_bertscore_f1\": np.mean(\n",
    "            bertscore.compute(predictions=predictions, references=targets, lang=\"en\")[\n",
    "                \"f1\"\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "metrics = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            compute_metrics(\n",
    "                dataset_dict[split][\"generated\"], dataset_dict[split][\"censored\"]\n",
    "            ),\n",
    "            index=[split],\n",
    "        )\n",
    "        for split in splits\n",
    "    ]\n",
    ")\n",
    "\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
