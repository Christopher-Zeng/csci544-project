{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    val: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 8679\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3720\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from IPython.display import display, Pretty\n",
    "\n",
    "with open(\"secrets.json\", \"r\") as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "\n",
    "dataset_dict = dataset.train_test_split(test_size=2 / 10, seed=42)\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"val\": dataset_dict[\"test\"],\n",
    "        **dataset_dict[\"train\"].train_test_split(test_size=3 / 10, seed=42),\n",
    "    }\n",
    ")\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer, input_text, target_text):\n",
    "    encoding = tokenizer(input_text, padding=\"longest\", pad_to_multiple_of=8)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        encoding[\"labels\"] = tokenizer(\n",
    "            target_text, padding=\"longest\", pad_to_multiple_of=8\n",
    "        )[\"input_ids\"]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "def get_traning_args(model_name):\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=\"outputs/\" + model_name + \"detox\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=20,\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=3,\n",
    "        gradient_accumulation_steps=64,\n",
    "        eval_accumulation_steps=128,\n",
    "        dataloader_num_workers=4,\n",
    "        predict_with_generate=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        remove_unused_columns=True,\n",
    "        optim=\"adamw_apex_fused\",\n",
    "        fp16=True,\n",
    "        fp16_opt_level=\"O2\",\n",
    "        tf32=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        push_to_hub=True,\n",
    "        hub_strategy=\"all_checkpoints\",\n",
    "        hub_model_id=model_name + \"-detox\",\n",
    "        hub_token=secrets[\"hub_token_write\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-base\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import os\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoding = dataset_dict.map(\n",
    "    lambda rec: encode(tokenizer, rec[\"original\"], rec[\"censored\"]),\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=get_traning_args(model_name),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    "    train_dataset=encoding[\"train\"],\n",
    "    eval_dataset=encoding[\"val\"],\n",
    ")\n",
    "\n",
    "training_output = trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "batched_inputs = dataset[\"original\"][:128]\n",
    "\n",
    "model_name = \"ChrisZeng/t5-base-detox\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_sequence = tokenizer(\n",
    "    batched_inputs, padding=\"longest\", pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output_sequence = model.generate(\n",
    "        input_ids=input_sequence[\"input_ids\"],\n",
    "        attention_mask=input_sequence[\"attention_mask\"],\n",
    "    )\n",
    "    \n",
    "generated = tokenizer.batch_decode(output_sequence, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    Text2TextGenerationPipeline,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "\n",
    "model_name = \"ChrisZeng/t5-base-detox\"\n",
    "\n",
    "pipe = Text2TextGenerationPipeline(\n",
    "    model=AutoModelForSeq2SeqLM.from_pretrained(model_name),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "    device=0,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = pipe(dataset[\"original\"], batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yet call out all Muslims for the acts of a few will get you pilloried.',\n",
       " 'This c> is c>. Who would read a book by ',\n",
       " \"You're an c>.\",\n",
       " \"Nincompoop, that's a nice one! I'm partial to \",\n",
       " \"testing purposes: you are an c> and i can't stand ignorant do\",\n",
       " 'Well c>, they drafted a guide. We should all be good now',\n",
       " 'I honestly cannot decide if these guys are complete c> or the most patriotic',\n",
       " 'Dear world: Just a reminder that none of these folks actually live in Oregon. Please remember',\n",
       " 'These parents are a bunch of c> crybabies. As a Lincoln',\n",
       " 'It would take a lot of practice and muscles control to vote with your c>',\n",
       " 'DARPA, the subdivision of the Defense Department in charge of devising Really Scary Shi',\n",
       " 'I wonder how much money taxpayers money PPS is going to waste on c>',\n",
       " 'Anyone who allows Obama to nominate him/her is throwing away a chance to ever be',\n",
       " 'Two Wild & c> Guys!',\n",
       " \"I was talking about the comments here---I wouldn't be so c> to\",\n",
       " 'Yeah, too bad... Oregon Live has 100 times the comments, including the c',\n",
       " 'c> this guy flag his post',\n",
       " \"Cassie, you'd better hope that the c> who picks up hit\",\n",
       " 'Tongue firmly in cheek. How did he manage to pass the background check to acquire',\n",
       " 'Yahoo!!! Finally someone calling this old c> to account. I have had the distinct',\n",
       " 'So, Ms. Ricci, you weren\\'t \"embarrassed',\n",
       " 'Wow. You prove the point made in the letter. Just how many wars would you like',\n",
       " 'You support Trump...an avowed c>. Why?',\n",
       " 'I used to love bike riding but now there are so many social c>, ',\n",
       " \"Aside of a few c> I don't see many san\",\n",
       " \"This guy is incredibly c>. He's going to be eaten alive in\",\n",
       " \"You're kind of a c>, aren't you?\",\n",
       " 'You are c>. Pure and simple.',\n",
       " 'You sir, are an c>.',\n",
       " \"Don't be c>. This is about Brown. But the jury could have\",\n",
       " 'So c> true!! \"Portlandia\" killed my sweet find',\n",
       " 'What a waste of time... this project hardly deserves this kind of coverage. 22 pictures',\n",
       " 'And you read it all, making you exponentially MORE c>.',\n",
       " 'liberal dementia, a social disease that knows no shame, often claims Reagan broke the unions',\n",
       " 'I have been saying this since that c> c> show came out.',\n",
       " 'looking at it...trying to understand how this profound c> is dominating this',\n",
       " 'Yes what a c> I imagine that there is a long line of lawyers',\n",
       " 'a person can only take so much, and when irresponsible neighbors let their',\n",
       " 'c>, social c> leeching off welfare, and liberals',\n",
       " \"The Donald's boasts claiming himself capable of being America's only choice for our\",\n",
       " 'Die yuppie c>.',\n",
       " 'What happened to the Willy Week? This revue is terrible. Also, associating the',\n",
       " \"I'm quite sure I've already read this article a few years ago, same\",\n",
       " 'Sounds like some of you are scared of c> and its obvious who that is!',\n",
       " \"It's obvious that the man hates c>, lol, I guess everyone\",\n",
       " 'What a shameful, c> anti-semitic comment.',\n",
       " 'actually, that first paragraph is a bastardization of William F Buckley quotes.',\n",
       " \"I'm going to smoke pot just to make this fair. Ten years ago we had just\",\n",
       " 'David and Charles Koch card is soooooo over played by the left it appears as ',\n",
       " 'c>, send him to ISIS.',\n",
       " 'The PC local liberal good old boy club permeates Eugene like the KKK of the',\n",
       " 'Red lights? When did they start that c>?',\n",
       " 'What a pile of c>. I love Bruce and I could write a',\n",
       " \"I just - I just can't tell if you're serious. It'\",\n",
       " 'Nice come-back to quite a c> comment.',\n",
       " 'creating additional taxes that removes money from the local economy and given to the highly inefficient government',\n",
       " 'It would be nice to know whom I am addressing but the RG editorial board only hide',\n",
       " 'c>!',\n",
       " \"You all need to quit talking about c> that is irrelevant. What's relevant\",\n",
       " 'Hate Kalis for this very reason. Same c> going on in the ',\n",
       " 'c> mister mayor!',\n",
       " 'No, they are c> and are ALLOWING THE FEDERAL G',\n",
       " 'Only a left wing c> could write such a ridiculous headline. How',\n",
       " 'Buy the c> thing for a low price and be done with this nonsense',\n",
       " 'I left him c>. And eff to the big c> ',\n",
       " 'I wont hold my breath a liberal bigot like you will ever take a',\n",
       " 'Actually, I do think voters are c> and deserve what they elect. If they',\n",
       " 'Problem: The deluded and c> leading the legislative majorities have a',\n",
       " \"What the c> is with this 'civil comments' horseshit? We\",\n",
       " 'The c> at Anchorage Daily News have cooked up a new censorship',\n",
       " 'The c> at Anchorage Daily News have cooked up a new censorship',\n",
       " 'year after year this criminal dysfunction continued, and knowing it was going on the county board, city',\n",
       " 'Miserable? Me? Over white guilt? No, not at all. How about you',\n",
       " 'Craig is a so-called conservative c> on the payroll to try and shutdown',\n",
       " 'Senator, I\\'m jaded. Too often \"We must invest in Alaska\\'s',\n",
       " 'If people want to be so c> to use this stuff let them kill them se',\n",
       " 'Socialism, FAILS EVERY TIME. Always has, always will. Welfare ',\n",
       " 'Well said. Another c> socialist failure. Bernie is proof that babies still get dropped',\n",
       " 'Had no idea there were so many c>, c>, ',\n",
       " 'Lumber sexual, metro sexual, ----sexual, sheesh does everything have',\n",
       " 'They later came out and said they had counted cards from Hillary supporters who had left early',\n",
       " 'I am 56 years old, I am not your c> junior pal. What you',\n",
       " '\"Free c> for everyone\" supporters have no clue what you\\'re talking about',\n",
       " 'Because donald trump is a c> with old family money;',\n",
       " 'No, not at all. Stop with your \"then you admit\" c>',\n",
       " 'Lela, for an educated lady and a writter you certainly seem ',\n",
       " 'All people had to do was read the pamplet that was mailed out explaining SB21',\n",
       " 'Exactly. These scandalous groups plan this c> while the average person knows not',\n",
       " 'You have your mind made up, so do I, facts do not interest you, so I',\n",
       " 'Microagression and liberal c>.',\n",
       " 'On the one hand liberal dementia demands we protect every last acre of \"farmland',\n",
       " 'Holy smokes, color me c>. I have lived in Alaska since I was',\n",
       " \"Exactly how are Ms. Dodd's mere words more harmful than the deep\",\n",
       " \"If you ain't white you ain't right...story of\",\n",
       " 'The gay community is the most oppressive organization out there and your comment reflects that! They',\n",
       " 'It appears Christgau has been reading some far left wing c> polls talk',\n",
       " \"I'm glad the RG published this c> ugly picture. As he\",\n",
       " 'the 47% DONKEY congress is in the same 11% boat.',\n",
       " 'c>',\n",
       " 'My god, how dare you confuse this person with simple facts! You have some nerve.',\n",
       " 'c> than a 5th grader...c>',\n",
       " 'They are c>. They will act in the same manner, c>',\n",
       " 'prosecute the c>.',\n",
       " 'I explained to Junior the father of conservatism was a pot head who advocated legalization',\n",
       " 'I explained exactly why you are an c> who has absolutely no idea what you are',\n",
       " 'A pot head telling the rest to stay off drugs, c>.',\n",
       " 'Red Light running with a wreck and/or injury should be a jailable offense.',\n",
       " \"Canada's ash my c>, its Alaska's ash!\",\n",
       " 'Eligible Americans appear to enjoy Medicaid, Medicare, Social Security, Unemployment Benefits,',\n",
       " 'Wow c> than c>,..Lol',\n",
       " 'c>!!!!!',\n",
       " 'And the reason they brought the drugs down there is because..... There are plenty of c',\n",
       " 'This is one of the c> things anti-gun people say: They list every',\n",
       " 'You proved my point. You are incapable of posting anything remotely intelligent. When your profound ',\n",
       " 'I sure hope that dog was able to bite that c> before he was',\n",
       " 'old soul, you post c> c> time and time again, and',\n",
       " 'Why not just c> him and put him in the freezer?',\n",
       " 'This is so c>. Take the lousy government out of our bodies,',\n",
       " \"I meant re: Young's class. He only yells at people who\",\n",
       " 'I expect such gutless cowardly behavior from you. I expect you to post ',\n",
       " 'c> for everyone! No thanks! Pay your own way.',\n",
       " 'What an c> you are.',\n",
       " 'Matthews pestered Trump as in badgered, hounded, nagged,',\n",
       " 'Margaret Conaway, thank you for your excellent letter! And yes Kelly is c>',\n",
       " 'Wow, this c> is regarded as journalism... that is what is wrong with',\n",
       " 'remove their clitoris and sew their vaginas closed... alah,',\n",
       " 'Redheads are a problem. Quick to take offense, quick to reach for the knife',\n",
       " 'hahaha Yeah we dont do c> like this....we are real criminals']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
