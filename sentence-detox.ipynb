{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "\n",
    "dataset_dict = dataset.train_test_split(test_size=2 / 10, seed=42)\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"eval\": dataset_dict[\"test\"],\n",
    "        **dataset_dict[\"train\"].train_test_split(test_size=3 / 8, seed=42),\n",
    "    }\n",
    ")\n",
    "\n",
    "for split, dataset in dataset_dict.items():\n",
    "    dataset.to_pandas().to_csv(\n",
    "        data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\"), index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 8679\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3720\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"secrets.json\", \"r\") as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "splits = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(\n",
    "            pd.read_csv(\n",
    "                data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\")\n",
    "            )\n",
    "        )\n",
    "        for split in splits\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer, input_text, target_text):\n",
    "    encoding = tokenizer(input_text)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        encoding[\"labels\"] = tokenizer(target_text)[\"input_ids\"]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "def get_traning_args(model_name):\n",
    "    model_name = model_name[model_name.find(\"/\") + 1 :]\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=\"outputs/\" + model_name + \"-detox\",\n",
    "        overwrite_output_dir=True,\n",
<<<<<<< HEAD
    "        num_train_epochs=20,\n",
    "        learning_rate=1e-5,\n",
=======
    "        num_train_epochs=10,\n",
    "        learning_rate=1e-6,\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=16,\n",
    "        eval_accumulation_steps=128,\n",
    "        dataloader_num_workers=3,\n",
    "        predict_with_generate=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        remove_unused_columns=True,\n",
    "        optim=\"adamw_apex_fused\",\n",
    "        bf16=True,\n",
    "        bf16_full_eval=True,\n",
    "        tf32=True,\n",
    "        gradient_checkpointing=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        push_to_hub=True,\n",
    "        hub_strategy=\"all_checkpoints\",\n",
    "        hub_model_id=model_name + \"-detox\",\n",
    "        hub_token=secrets[\"hub_token_write\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "  warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb5cc10e178439dabc48c47e2738bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd8003f855547e78f9b70fcfbbeacae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5dadc22d614fd3a78afa9addea5ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720a2ae3f8344d98aa57d173975492f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1781ae12a0c406a86601b9e60c45838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "5f0f1054052f4931b62db715f6840d04",
=======
       "model_id": "65b700dc60724806a9c4fc685aae078f",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8679 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "4b5e31057aa04656835d6026d09d06ec",
=======
       "model_id": "010d2a75cf00416eb64fb44de38fa7d7",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "1aa5e37849bc413cb37c9af4d1627120",
=======
       "model_id": "bd32390a13ae455fa78c161a2e75ed93",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3720 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/home/chris-zeng/csci544-project/outputs/bart-base-detox is already a clone of https://huggingface.co/ChrisZeng/bart-base-detox. Make sure you pull the latest changes with `repo.git_pull()`.\n",
=======
      "Cloning https://huggingface.co/ChrisZeng/bart-base-detox into local empty directory.\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.add_tokens(\"<CSD>\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.update({\"use_cache\": False})\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "encoding = dataset_dict.map(\n",
    "    lambda rec: encode(tokenizer, rec[\"original\"], rec[\"censored\"]),\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=get_traning_args(model_name),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer, model=model, padding=\"longest\", pad_to_multiple_of=8\n",
    "    ),\n",
    "    train_dataset=encoding[\"train\"],\n",
    "    eval_dataset=encoding[\"eval\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Loading model from outputs/bart-base-detox/checkpoint-1350).\n",
=======
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running training *****\n",
      "  Num examples = 8679\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 2700\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 10\n",
      "  Continuing training from global step 1350\n",
      "  Will skip the first 10 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83000610706144dfa1397e39549be91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
<<<<<<< HEAD
       "      <progress value='2700' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2700/2700 36:35, Epoch 19/20]\n",
=======
       "      <progress value='679' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 679/1350 18:34 < 18:24, 0.61 it/s, Epoch 5.02/10]\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <td>10</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.185777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.181999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.182661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.184280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.183421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>0.182027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.183651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.181359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>0.181473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>0.181861</td>\n",
=======
       "      <td>0</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.201793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>0.187287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.180199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.187119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.192630</td>\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
<<<<<<< HEAD
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-1485\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-1485/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-1485/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-1485/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-1485/special_tokens_map.json\n",
=======
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-135\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-135/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-135/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-135/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-135/special_tokens_map.json\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
<<<<<<< HEAD
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-1620\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-1620/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-1620/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-1620/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-1620/special_tokens_map.json\n",
=======
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-270\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-270/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-270/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-270/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-270/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
<<<<<<< HEAD
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-1755\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-1755/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-1755/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-1755/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-1755/special_tokens_map.json\n",
=======
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-405\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-405/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-405/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-405/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-405/special_tokens_map.json\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
<<<<<<< HEAD
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-1890\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-1890/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-1890/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-1890/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-1890/special_tokens_map.json\n",
=======
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-540\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-540/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-540/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-540/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-540/special_tokens_map.json\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
<<<<<<< HEAD
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2025\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2025/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2025/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2025/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2025/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2160\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2160/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2160/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2160/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2160/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2295\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2295/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2295/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2295/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2295/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2430\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2430/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2430/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2430/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2430/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2565\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2565/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2565/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2565/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2565/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-2700\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-2700/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-2700/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-2700/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-2700/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/bart-base-detox/checkpoint-2430 (score: 0.1813586801290512).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2700, training_loss=0.08368111080593534, metrics={'train_runtime': 2197.9803, 'train_samples_per_second': 78.973, 'train_steps_per_second': 1.228, 'total_flos': 1.075972393746432e+16, 'train_loss': 0.08368111080593534, 'epoch': 20.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
=======
      "Saving model checkpoint to outputs/bart-base-detox/checkpoint-675\n",
      "Configuration saved in outputs/bart-base-detox/checkpoint-675/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/checkpoint-675/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/checkpoint-675/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/checkpoint-675/special_tokens_map.json\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/chris-zeng/csci544-project/sentence-detox.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py:1365\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1362'>1363</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1363'>1364</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1364'>1365</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1366'>1367</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1367'>1368</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1368'>1369</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1369'>1370</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1370'>1371</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1371'>1372</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1372'>1373</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py:1950\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1946'>1947</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1948'>1949</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m-> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1949'>1950</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1950'>1951</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/trainer.py?line=1951'>1952</a>\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/torch/_tensor.py:399\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/_tensor.py?line=389'>390</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/_tensor.py?line=390'>391</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/_tensor.py?line=391'>392</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/_tensor.py?line=392'>393</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/_tensor.py?line=396'>397</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/_tensor.py?line=397'>398</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/_tensor.py?line=398'>399</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Saving model checkpoint to outputs/bart-base-detox\n",
      "Configuration saved in outputs/bart-base-detox/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/bart-base-detox/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n",
      "Saving model checkpoint to outputs/bart-base-detox\n",
      "Configuration saved in outputs/bart-base-detox/config.json\n",
      "Model weights saved in outputs/bart-base-detox/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/bart-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/bart-base-detox/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2cad4e31544d60ab30bf5c126ad583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/optimizer.pt:   0%|          | 32.0k/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f874399bf04840ed8364bd6c5a423e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/scheduler.pt: 100%|##########| 623/623 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db12acfb411a4bdbb0a3283467190ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/pytorch_model.bin:   0%|          | 32.0k/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6d8466e9ef4e5a94928144da05742e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/scaler.pt: 100%|##########| 559/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f7be5ee4944ad192b23053fa6d3faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-2700/rng_state.pth: 100%|##########| 14.2k/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/ChrisZeng/bart-base-detox\n",
      "   22aac39..c9e4d58  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "To https://huggingface.co/ChrisZeng/bart-base-detox\n",
      "   c9e4d58..c2af807  main -> main\n",
=======
      "Saving model checkpoint to outputs/t5-base-detox\n",
      "Configuration saved in outputs/t5-base-detox/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-base-detox/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-base-detox/spiece.model\n",
      "Saving model checkpoint to outputs/t5-base-detox\n",
      "Configuration saved in outputs/t5-base-detox/config.json\n",
      "Model weights saved in outputs/t5-base-detox/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-base-detox/spiece.model\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "To https://huggingface.co/ChrisZeng/t5-base-detox\n",
      "   6aba3b3..03ecfe9  main -> main\n",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/3d051d400035f2ca53580da38002eb0d2b7e188715710019c196e53ce2863c9c.4216d65d19bd74d36444d35fc3c0231b7f18539c9abaa91f81207dcadbf71eb9\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"ChrisZeng/t5-v1_1-base-detox\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32101\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/4c20b41026a8477b1db11f1a60fa8b54001dea569cf40a81a09c9cf955cef921.e1368003b4dd9328d37136550d0f086f2b1ee2d5691ac5494ed10f93a3ea8c6e\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ChrisZeng/t5-v1_1-base-detox.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/spiece.model from cache at /home/chris-zeng/.cache/huggingface/transformers/fd81b59c49c7368bd8b067b271985d624fc0ff3a2031041d93c44e1ad1a0d57b.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/tokenizer.json from cache at /home/chris-zeng/.cache/huggingface/transformers/5b45c8a95d30db679b4dc156d04ca211868050290b548c9dfd4e8de6e313f4a4.7548eb378c431c6b8417c829064c28670d7d1f35cd6fa88bb5de9da11fbe9dac\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/added_tokens.json from cache at /home/chris-zeng/.cache/huggingface/transformers/42c42f5e1b33129c0789a892f9313d0928e65a7d4157ba33fc1a42464975e044.e6cf36755a4a23d7c5dc882dc268bb487808b6233a7cde8384de4c94b0bbd102\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/special_tokens_map.json from cache at /home/chris-zeng/.cache/huggingface/transformers/d435947d720628841707d02794c46edbaf8cf7adb12203d40e24908eb93c3428.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/ChrisZeng/t5-v1_1-base-detox/resolve/main/tokenizer_config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/8ca641d3ceb800bd8093b1e6e7dde17c6bacfd70698429d3810bc68b514f8af0.e4f38c0fb7cab0ed96b2b6e2b88a3d32d8fe5b2cc267d0dd8816db923e309305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "bc86d0be3ea94fadb56b2933af44cc6a",
=======
       "model_id": "861e6d7455b444c38a16603ced7b465f",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "67f34ce17ca34448bc1b09f7ea07d7a3",
=======
       "model_id": "370d9fc85661473f84b83d450d9cae20",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "366c88687d5d43d79ff12ff37b97eaa4",
=======
       "model_id": "9ddeb39a194747d1b4ff00d4ff2b033e",
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def detox(tokenizer, model_buffered, batched_inputs):\n",
    "    input_sequence_buffered = tokenizer(\n",
    "        batched_inputs, padding=\"longest\", pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_sequence = model_buffered.generate(\n",
    "            input_ids=input_sequence_buffered[\"input_ids\"],\n",
    "            attention_mask=input_sequence_buffered[\"attention_mask\"],\n",
    "        )\n",
    "    del input_sequence_buffered\n",
    "    generated = [\n",
    "        \"<CSD>\" if len(generated) == 0 else generated\n",
    "        for generated in tokenizer.batch_decode(\n",
    "            output_sequence, skip_special_tokens=True\n",
    "        )\n",
    "    ]\n",
    "    return {\"generated\": generated}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "splits = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(\n",
    "            pd.read_csv(\n",
    "                data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\")\n",
    "            )\n",
    "        )\n",
    "        for split in splits\n",
    "    }\n",
    ")\n",
    "\n",
    "model_name = \"ChrisZeng/t5-v1_1-base-detox\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_buffered = model.to(\"cuda\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset_dict = dataset_dict.map(\n",
    "    lambda rec: detox(tokenizer, model_buffered, rec[\"original\"]),\n",
    "    keep_in_memory=True,\n",
    "    batched=True,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "del model_buffered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /home/chris-zeng/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /home/chris-zeng/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /home/chris-zeng/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n"
     ]
    },
    {
<<<<<<< HEAD
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>exact_match_rate</th>\n",
       "      <th>mean_bertscore_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.574129</td>\n",
       "      <td>0.498884</td>\n",
       "      <td>0.569738</td>\n",
       "      <td>0.569856</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.909635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval</th>\n",
       "      <td>0.584921</td>\n",
       "      <td>0.506518</td>\n",
       "      <td>0.580766</td>\n",
       "      <td>0.581342</td>\n",
       "      <td>0.080323</td>\n",
       "      <td>0.910855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.575102</td>\n",
       "      <td>0.496492</td>\n",
       "      <td>0.570006</td>\n",
       "      <td>0.570352</td>\n",
       "      <td>0.079839</td>\n",
       "      <td>0.909516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rouge1    rouge2    rougeL  rougeLsum  exact_match_rate  \\\n",
       "train  0.574129  0.498884  0.569738   0.569856          0.091600   \n",
       "eval   0.584921  0.506518  0.580766   0.581342          0.080323   \n",
       "test   0.575102  0.496492  0.570006   0.570352          0.079839   \n",
       "\n",
       "       mean_bertscore_f1  \n",
       "train           0.909635  \n",
       "eval            0.910855  \n",
       "test            0.909516  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
=======
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 674.00 MiB (GPU 0; 8.00 GiB total capacity; 6.34 GiB already allocated; 0 bytes free; 6.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/chris-zeng/csci544-project/sentence-detox.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(predictions, targets):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=12'>13</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=13'>14</a>\u001b[0m             key: value\u001b[39m.\u001b[39mmid\u001b[39m.\u001b[39mfmeasure\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=26'>27</a>\u001b[0m         ),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=27'>28</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=30'>31</a>\u001b[0m metrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=31'>32</a>\u001b[0m     [\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=32'>33</a>\u001b[0m         pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=33'>34</a>\u001b[0m             compute_metrics(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=34'>35</a>\u001b[0m                 dataset_dict[split][\u001b[39m\"\u001b[39m\u001b[39mgenerated\u001b[39m\u001b[39m\"\u001b[39m], dataset_dict[split][\u001b[39m\"\u001b[39m\u001b[39mcensored\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=35'>36</a>\u001b[0m             ),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=36'>37</a>\u001b[0m             index\u001b[39m=\u001b[39m[split],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=37'>38</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=38'>39</a>\u001b[0m         \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m splits\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=39'>40</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=42'>43</a>\u001b[0m metrics\n",
      "\u001b[1;32m/home/chris-zeng/csci544-project/sentence-detox.ipynb Cell 8'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(predictions, targets):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=12'>13</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=13'>14</a>\u001b[0m             key: value\u001b[39m.\u001b[39mmid\u001b[39m.\u001b[39mfmeasure\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=26'>27</a>\u001b[0m         ),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=27'>28</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=30'>31</a>\u001b[0m metrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=31'>32</a>\u001b[0m     [\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=32'>33</a>\u001b[0m         pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=33'>34</a>\u001b[0m             compute_metrics(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=34'>35</a>\u001b[0m                 dataset_dict[split][\u001b[39m\"\u001b[39;49m\u001b[39mgenerated\u001b[39;49m\u001b[39m\"\u001b[39;49m], dataset_dict[split][\u001b[39m\"\u001b[39;49m\u001b[39mcensored\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=35'>36</a>\u001b[0m             ),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=36'>37</a>\u001b[0m             index\u001b[39m=\u001b[39m[split],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=37'>38</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=38'>39</a>\u001b[0m         \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m splits\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=39'>40</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=42'>43</a>\u001b[0m metrics\n",
      "\u001b[1;32m/home/chris-zeng/csci544-project/sentence-detox.ipynb Cell 8'\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(predictions, targets):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=12'>13</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=13'>14</a>\u001b[0m             key: value\u001b[39m.\u001b[39mmid\u001b[39m.\u001b[39mfmeasure\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=14'>15</a>\u001b[0m             \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m rouge\u001b[39m.\u001b[39mcompute(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=15'>16</a>\u001b[0m                 predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mtargets\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=16'>17</a>\u001b[0m             )\u001b[39m.\u001b[39mitems()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=17'>18</a>\u001b[0m         },\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexact_match_rate\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.01\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=19'>20</a>\u001b[0m         \u001b[39m*\u001b[39m exact_match\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mtargets)[\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=20'>21</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexact_match\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=21'>22</a>\u001b[0m         ],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=22'>23</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmean_bertscore_f1\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmean(\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=23'>24</a>\u001b[0m             bertscore\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mpredictions, references\u001b[39m=\u001b[39;49mtargets, lang\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=24'>25</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=25'>26</a>\u001b[0m             ]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=26'>27</a>\u001b[0m         ),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcsci-ai/home/chris-zeng/csci544-project/sentence-detox.ipynb#ch0000007vscode-remote?line=27'>28</a>\u001b[0m     }\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/datasets/metric.py:430\u001b[0m, in \u001b[0;36mMetric.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/datasets/metric.py?line=427'>428</a>\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/datasets/metric.py?line=428'>429</a>\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/datasets/metric.py?line=429'>430</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/datasets/metric.py?line=431'>432</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/datasets/metric.py?line=432'>433</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py:179\u001b[0m, in \u001b[0;36mBERTScore._compute\u001b[0;34m(self, predictions, references, lang, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=164'>165</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcached_bertscorer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer\u001b[39m.\u001b[39mhash \u001b[39m!=\u001b[39m hashcode:\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=165'>166</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer \u001b[39m=\u001b[39m scorer(\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=166'>167</a>\u001b[0m             model_type\u001b[39m=\u001b[39mmodel_type,\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=167'>168</a>\u001b[0m             num_layers\u001b[39m=\u001b[39mnum_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=175'>176</a>\u001b[0m             baseline_path\u001b[39m=\u001b[39mbaseline_path,\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=176'>177</a>\u001b[0m         )\n\u001b[0;32m--> <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=178'>179</a>\u001b[0m (P, R, F) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcached_bertscorer\u001b[39m.\u001b[39;49mscore(\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=179'>180</a>\u001b[0m     cands\u001b[39m=\u001b[39;49mpredictions,\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=180'>181</a>\u001b[0m     refs\u001b[39m=\u001b[39;49mreferences,\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=181'>182</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=182'>183</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=183'>184</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=184'>185</a>\u001b[0m output_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=185'>186</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m: P\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=186'>187</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m: R\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=187'>188</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=188'>189</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhashcode\u001b[39m\u001b[39m\"\u001b[39m: hashcode,\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=189'>190</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///home/chris-zeng/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=190'>191</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output_dict\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py:213\u001b[0m, in \u001b[0;36mBERTScorer.score\u001b[0;34m(self, cands, refs, verbose, batch_size, return_hash)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=209'>210</a>\u001b[0m     idf_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39msep_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=210'>211</a>\u001b[0m     idf_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mcls_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=212'>213</a>\u001b[0m all_preds \u001b[39m=\u001b[39m bert_cos_score_idf(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=213'>214</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=214'>215</a>\u001b[0m     refs,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=215'>216</a>\u001b[0m     cands,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=216'>217</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=217'>218</a>\u001b[0m     idf_dict,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=218'>219</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=219'>220</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=220'>221</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=221'>222</a>\u001b[0m     all_layers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_layers,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=222'>223</a>\u001b[0m )\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=224'>225</a>\u001b[0m \u001b[39mif\u001b[39;00m ref_group_boundaries \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/scorer.py?line=225'>226</a>\u001b[0m     max_preds \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py:528\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[0;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=525'>526</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_start \u001b[39min\u001b[39;00m iter_range:\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=526'>527</a>\u001b[0m     sen_batch \u001b[39m=\u001b[39m sentences[batch_start : batch_start \u001b[39m+\u001b[39m batch_size]\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=527'>528</a>\u001b[0m     embs, masks, padded_idf \u001b[39m=\u001b[39m get_bert_embedding(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=528'>529</a>\u001b[0m         sen_batch, model, tokenizer, idf_dict, device\u001b[39m=\u001b[39;49mdevice, all_layers\u001b[39m=\u001b[39;49mall_layers\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=529'>530</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=530'>531</a>\u001b[0m     embs \u001b[39m=\u001b[39m embs\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=531'>532</a>\u001b[0m     masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py:407\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[0;34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=404'>405</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=405'>406</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(all_sens), batch_size):\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=406'>407</a>\u001b[0m         batch_embedding \u001b[39m=\u001b[39m bert_encode(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=407'>408</a>\u001b[0m             model, padded_sens[i : i \u001b[39m+\u001b[39;49m batch_size], attention_mask\u001b[39m=\u001b[39;49mmask[i : i \u001b[39m+\u001b[39;49m batch_size], all_layers\u001b[39m=\u001b[39;49mall_layers,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=408'>409</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=409'>410</a>\u001b[0m         embeddings\u001b[39m.\u001b[39mappend(batch_embedding)\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=410'>411</a>\u001b[0m         \u001b[39mdel\u001b[39;00m batch_embedding\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py:318\u001b[0m, in \u001b[0;36mbert_encode\u001b[0;34m(model, x, attention_mask, all_layers)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=315'>316</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=316'>317</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=317'>318</a>\u001b[0m     out \u001b[39m=\u001b[39m model(x, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_hidden_states\u001b[39m=\u001b[39;49mall_layers)\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=318'>319</a>\u001b[0m \u001b[39mif\u001b[39;00m all_layers:\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/bert_score/utils.py?line=319'>320</a>\u001b[0m     emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:850\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=840'>841</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=842'>843</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=843'>844</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=844'>845</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=847'>848</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=848'>849</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=849'>850</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=850'>851</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=851'>852</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=852'>853</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=853'>854</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=854'>855</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=855'>856</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=856'>857</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=857'>858</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=858'>859</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=859'>860</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=860'>861</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=861'>862</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=862'>863</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:526\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=516'>517</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=517'>518</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=518'>519</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=522'>523</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=523'>524</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=524'>525</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=525'>526</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=526'>527</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=527'>528</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=528'>529</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=529'>530</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=530'>531</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=531'>532</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=532'>533</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=533'>534</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=535'>536</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=536'>537</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:412\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=399'>400</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=400'>401</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=401'>402</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=408'>409</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=409'>410</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=410'>411</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=411'>412</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=412'>413</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=413'>414</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=414'>415</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=415'>416</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=416'>417</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=417'>418</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=418'>419</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=420'>421</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:339\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=328'>329</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=329'>330</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=330'>331</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=336'>337</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=337'>338</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=338'>339</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=339'>340</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=340'>341</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=341'>342</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=342'>343</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=343'>344</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=344'>345</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=345'>346</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=346'>347</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=347'>348</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=348'>349</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:259\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=255'>256</a>\u001b[0m         relative_position_scores_key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbhrd,lrd->bhlr\u001b[39m\u001b[39m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=256'>257</a>\u001b[0m         attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m relative_position_scores_query \u001b[39m+\u001b[39m relative_position_scores_key\n\u001b[0;32m--> <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=258'>259</a>\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_head_size)\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=259'>260</a>\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=260'>261</a>\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py?line=261'>262</a>\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 674.00 MiB (GPU 0; 8.00 GiB total capacity; 6.34 GiB already allocated; 0 bytes free; 6.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
>>>>>>> bd8d03a8a67049ea2ccc274b70f256e4ceb0e582
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Pretty\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "exact_match = load_metric(\"exact_match\")\n",
    "bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, targets):\n",
    "    return {\n",
    "        **{\n",
    "            key: value.mid.fmeasure\n",
    "            for key, value in rouge.compute(\n",
    "                predictions=predictions, references=targets\n",
    "            ).items()\n",
    "        },\n",
    "        \"exact_match_rate\": 0.01\n",
    "        * exact_match.compute(predictions=predictions, references=targets)[\n",
    "            \"exact_match\"\n",
    "        ],\n",
    "        \"mean_bertscore_f1\": np.mean(\n",
    "            bertscore.compute(predictions=predictions, references=targets, lang=\"en\")[\n",
    "                \"f1\"\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "metrics = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            compute_metrics(\n",
    "                dataset_dict[split][\"generated\"], dataset_dict[split][\"censored\"]\n",
    "            ),\n",
    "            index=[split],\n",
    "        )\n",
    "        for split in splits\n",
    "    ]\n",
    ")\n",
    "\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
