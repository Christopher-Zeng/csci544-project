{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "\n",
    "dataset_dict = dataset.train_test_split(test_size=2 / 10, seed=42)\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"eval\": dataset_dict[\"test\"],\n",
    "        **dataset_dict[\"train\"].train_test_split(test_size=3 / 10, seed=42),\n",
    "    }\n",
    ")\n",
    "\n",
    "for split, dataset in dataset_dict.items():\n",
    "    dataset.to_pandas().to_csv(\n",
    "        data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\"), index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 8679\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3720\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"secrets.json\", \"r\") as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "splits = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(\n",
    "            pd.read_csv(\n",
    "                data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\")\n",
    "            )\n",
    "        )\n",
    "        for split in splits\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer, input_text, target_text):\n",
    "    encoding = tokenizer(input_text)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        encoding[\"labels\"] = tokenizer(target_text)[\"input_ids\"]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "def get_traning_args(model_name):\n",
    "    model_name = model_name[model_name.find(\"/\") + 1 :]\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=\"outputs/\" + model_name + \"-detox\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=10,\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=16,\n",
    "        eval_accumulation_steps=128,\n",
    "        dataloader_num_workers=3,\n",
    "        predict_with_generate=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        remove_unused_columns=True,\n",
    "        optim=\"adamw_apex_fused\",\n",
    "        bf16=True,\n",
    "        bf16_full_eval=True,\n",
    "        tf32=True,\n",
    "        gradient_checkpointing=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        push_to_hub=True,\n",
    "        hub_strategy=\"all_checkpoints\",\n",
    "        hub_model_id=model_name + \"-detox\",\n",
    "        hub_token=secrets[\"hub_token_write\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "  warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4385133cb64ea5a7abac52a3a88784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8679 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725d26cad5d7418481e7b9b27d015a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d565630a794020b74e03d0efbac224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3720 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/ChrisZeng/t5-v1_1-base-detox into local empty directory.\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/t5-v1_1-base\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.add_tokens(\"<CSD>\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.update({\"use_cache\": False})\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "encoding = dataset_dict.map(\n",
    "    lambda rec: encode(tokenizer, rec[\"original\"], rec[\"censored\"]),\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=get_traning_args(model_name),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer, model=model, padding=\"longest\", pad_to_multiple_of=8\n",
    "    ),\n",
    "    train_dataset=encoding[\"train\"],\n",
    "    eval_dataset=encoding[\"eval\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running training *****\n",
      "  Num examples = 8679\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 1350\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 1:08:47, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.971900</td>\n",
       "      <td>2.031940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.943800</td>\n",
       "      <td>0.642404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>0.438422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.669400</td>\n",
       "      <td>0.345574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.533600</td>\n",
       "      <td>0.281917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.243061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.231866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.225216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.227114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.226963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-135\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-135/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-135/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-135/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-135/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-135/spiece.model\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-270\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-270/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-270/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-270/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-270/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-270/spiece.model\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-405\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-405/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-405/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-405/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-405/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-405/spiece.model\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-540\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-540/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-540/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-540/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-540/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-540/spiece.model\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-675\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-675/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-675/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-675/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-675/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-675/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-810\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-810/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-810/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-810/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-810/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-810/spiece.model\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-945\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-945/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-945/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-945/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-945/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-945/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-1080\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-1080/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-1080/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-1080/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-1080/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-1080/spiece.model\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-1215\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-1215/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-1215/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-1215/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-1215/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-1215/spiece.model\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: censored, original.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox/checkpoint-1350\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/checkpoint-1350/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/checkpoint-1350/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/checkpoint-1350/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/checkpoint-1350/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/checkpoint-1350/spiece.model\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/t5-v1_1-base-detox/checkpoint-1080 (score: 0.22521649301052094).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1350, training_loss=1.496486166494864, metrics={'train_runtime': 4130.9442, 'train_samples_per_second': 21.01, 'train_steps_per_second': 0.327, 'total_flos': 1.3138971617968128e+16, 'train_loss': 1.496486166494864, 'epoch': 10.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/t5-v1_1-base-detox\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/config.json\n",
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "Saving model checkpoint to outputs/t5-v1_1-base-detox\n",
      "Configuration saved in outputs/t5-v1_1-base-detox/config.json\n",
      "Model weights saved in outputs/t5-v1_1-base-detox/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/t5-v1_1-base-detox/tokenizer_config.json\n",
      "Special tokens file saved in outputs/t5-v1_1-base-detox/special_tokens_map.json\n",
      "Copy vocab file to outputs/t5-v1_1-base-detox/spiece.model\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "To https://huggingface.co/ChrisZeng/t5-v1_1-base-detox\n",
      "   9320141..aa06bfa  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris-zeng/csci544-project/apex-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6cb66c8b00445db546f4e098660b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee82ab80fae42eabd99e5cb705bb23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665ddf5032ba445d9a4174a57f47a1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def detox(tokenizer, model_buffered, batched_inputs):\n",
    "    input_sequence_buffered = tokenizer(\n",
    "        batched_inputs, padding=\"longest\", pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_sequence = model_buffered.generate(\n",
    "            input_ids=input_sequence_buffered[\"input_ids\"],\n",
    "            attention_mask=input_sequence_buffered[\"attention_mask\"],\n",
    "        )\n",
    "    del input_sequence_buffered\n",
    "    generated = [\n",
    "        \"<CSD>\" if len(generated) == 0 else generated\n",
    "        for generated in tokenizer.batch_decode(\n",
    "            output_sequence, skip_special_tokens=True\n",
    "        )\n",
    "    ]\n",
    "    return {\"generated\": generated}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "splits = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(\n",
    "            pd.read_csv(\n",
    "                data_path + \"/\" + filename.replace(\".csv\", \"_\" + split + \".csv\")\n",
    "            )\n",
    "        )\n",
    "        for split in splits\n",
    "    }\n",
    ")\n",
    "\n",
    "model_name = \"ChrisZeng/t5-v1_1-base-detox\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_buffered = model.to(\"cuda\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset_dict = dataset_dict.map(\n",
    "    lambda rec: detox(tokenizer, model_buffered, rec[\"original\"]),\n",
    "    keep_in_memory=True,\n",
    "    batched=True,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "del model_buffered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>exact_match_rate</th>\n",
       "      <th>mean_bertscore_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.573827</td>\n",
       "      <td>0.498888</td>\n",
       "      <td>0.569586</td>\n",
       "      <td>0.569978</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.909630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval</th>\n",
       "      <td>0.584910</td>\n",
       "      <td>0.506094</td>\n",
       "      <td>0.580824</td>\n",
       "      <td>0.580941</td>\n",
       "      <td>0.080323</td>\n",
       "      <td>0.910862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.574105</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.570146</td>\n",
       "      <td>0.570905</td>\n",
       "      <td>0.079839</td>\n",
       "      <td>0.909516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rouge1    rouge2    rougeL  rougeLsum  exact_match_rate  \\\n",
       "train  0.573827  0.498888  0.569586   0.569978          0.091600   \n",
       "eval   0.584910  0.506094  0.580824   0.580941          0.080323   \n",
       "test   0.574105  0.496000  0.570146   0.570905          0.079839   \n",
       "\n",
       "       mean_bertscore_f1  \n",
       "train           0.909630  \n",
       "eval            0.910862  \n",
       "test            0.909516  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Pretty\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "exact_match = load_metric(\"exact_match\")\n",
    "bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, targets):\n",
    "    return {\n",
    "        **{\n",
    "            key: value.mid.fmeasure\n",
    "            for key, value in rouge.compute(\n",
    "                predictions=predictions, references=targets\n",
    "            ).items()\n",
    "        },\n",
    "        \"exact_match_rate\": 0.01\n",
    "        * exact_match.compute(predictions=predictions, references=targets)[\n",
    "            \"exact_match\"\n",
    "        ],\n",
    "        \"mean_bertscore_f1\": np.mean(\n",
    "            bertscore.compute(predictions=predictions, references=targets, lang=\"en\")[\n",
    "                \"f1\"\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "metrics = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            compute_metrics(\n",
    "                dataset_dict[split][\"generated\"], dataset_dict[split][\"censored\"]\n",
    "            ),\n",
    "            index=[split],\n",
    "        )\n",
    "        for split in splits\n",
    "    ]\n",
    ")\n",
    "\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
