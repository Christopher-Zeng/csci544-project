{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"secrets.json\", \"r\") as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "\n",
    "dataset_dict = dataset.train_test_split(test_size=2 / 10, seed=42)\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"val\": dataset_dict[\"test\"],\n",
    "        **dataset_dict[\"train\"].train_test_split(test_size=3 / 10, seed=42),\n",
    "    }\n",
    ")\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer, input_text, target_text):\n",
    "    encoding = tokenizer(input_text, padding=\"longest\", pad_to_multiple_of=8)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        encoding[\"labels\"] = tokenizer(\n",
    "            target_text, padding=\"longest\", pad_to_multiple_of=8\n",
    "        )[\"input_ids\"]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def preprocess(tokenizer, record):\n",
    "    return {\"censored\": record[\"censored\"].replace(\"<c>\", \"<censored>\")}\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "def get_traning_args(model_name):\n",
    "    model_name = model_name[model_name.find('/'):]\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=\"outputs/\" + model_name + \"-detox\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=20,\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=3,\n",
    "        gradient_accumulation_steps=64,\n",
    "        eval_accumulation_steps=128,\n",
    "        dataloader_num_workers=4,\n",
    "        predict_with_generate=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        remove_unused_columns=True,\n",
    "        optim=\"adamw_apex_fused\",\n",
    "        fp16=True,\n",
    "        fp16_opt_level=\"O2\",\n",
    "        tf32=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        push_to_hub=True,\n",
    "        hub_strategy=\"all_checkpoints\",\n",
    "        hub_model_id=model_name + \"-detox\",\n",
    "        hub_token=secrets[\"hub_token_write\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/t5-v1_1-base\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import os\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"google/t5-v1_1-base\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import os\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.add_tokens([\"<censored>\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "encoding = dataset_dict.map(lambda rec: preprocess(tokenizer, rec)).map(\n",
    "    lambda rec: encode(tokenizer, rec[\"original\"], rec[\"censored\"]),\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "\n",
    "encoding = dataset_dict.map(lambda rec: preprocess(tokenizer, rec)).map(\n",
    "    lambda rec: encode(tokenizer, rec[\"original\"], rec[\"censored\"]),\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=get_traning_args(model_name),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    "    train_dataset=encoding[\"train\"],\n",
    "    eval_dataset=encoding[\"val\"],\n",
    ")\n",
    "\n",
    "training_output = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "def detox(tokenizer, model, batched_inputs, batched_targets):\n",
    "    input_sequence = tokenizer(\n",
    "        batched_inputs, padding=\"longest\", pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_sequence = model.generate(\n",
    "            input_ids=input_sequence[\"input_ids\"],\n",
    "            attention_mask=input_sequence[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.batch_decode(output_sequence, skip_special_tokens=True)\n",
    "    batched_targets = [\n",
    "        sentence.replace(\"<c>\", tokenizer.unk_token) for sentence in batched_targets\n",
    "    ]\n",
    "    target_sequence = tokenizer(\n",
    "        batched_targets, padding=\"longest\", pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    "    )\n",
    "    generated_target = tokenizer.batch_decode(\n",
    "        target_sequence[\"input_ids\"], skip_special_tokens=True\n",
    "    )\n",
    "    return {\n",
    "        \"generated\": generated,\n",
    "        \"generated_target\": generated_target,\n",
    "    }\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "\n",
    "model_name = \"outputs/t5-basedetox\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda rec: detox(tokenizer, model, rec[\"original\"], rec[\"censored\"]),\n",
    "    keep_in_memory=True,\n",
    "    batched=True,\n",
    "    batch_size=128,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from IPython.display import display, Pretty\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "exact_match = load_metric(\"exact_match\")\n",
    "bertscore = load_metric(\"bertscore\")\n",
    "{\n",
    "    **rouge.compute(\n",
    "        predictions=dataset[\"generated\"], references=dataset[\"generated_target\"],\n",
    "    ),\n",
    "    **exact_match.compute(\n",
    "        predictions=dataset[\"generated\"], references=dataset[\"generated_target\"],\n",
    "    ),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
