{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hub_token_write': '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'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from IPython.display import display, Pretty \n",
    "\n",
    "with open(\"secrets.json\", \"r\") as secrets_file:\n",
    "    secrets = json.load(secrets_file)\n",
    "display(secrets)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "electra_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/electra-large-discriminator\", normalization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4b7a38c31c4c02b6466407f91599ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34fc8da87394456baa5ed6c1659dd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6add7692e49149d69e70b7b9bce2f8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383fbf99401240a68362fb96184c53e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e792a4d402d24f76813a5fcce503be53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44028cbb83974780993a54d8d0318184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347af5b8626641b5ba1310bc7a2b8ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b94323423284cd990211e3ed53304cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6905c0c7ade9441e8a9503827768e7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c871b3040730458991f494dabda1c202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b383b6d08354c1ea15833bea4dd06da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e990dd6459384b80aee5a4fe20aa78c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45ddaca63454aaabe86bfd8c98bf49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7bf029bd7a412d9ffd199377d82c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3705c3c4d2d4409bba94b2c2f0b4d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff059cdf61594715add074101f612d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801498dea0b74df79cff06219fcb3530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60888c22d25740bd837bf6cfba2c1ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d9e773710d47d1b42c299dcfb3bd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f7a7aa072b4d26b04b58e4aef849b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92433af9a1748de8daf058b84d6b6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e07f7849e5446e79f6fa9f81738b9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5d22dd40eb41e2b1fa763282e2d0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20916 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13db58555af4d2689931b4059a0a584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2324 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09572fc217554d89aa254f873a00f93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef2b96d3f444d6e8223ed93f2d80a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f04d7bc003949639ead68057c800307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2970 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e2ef29b5b84493b46ec97f2d911f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11916 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f5903a21da4dbb8149f34eb977b89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1324 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f531a9a13730463ca4a05bba290f3405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/860 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_merged: Dataset({\n",
       "        features: ['premise', 'label_categoricals', 'hypothesis', 'task', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 20916\n",
       "    })\n",
       "    val_merged: Dataset({\n",
       "        features: ['premise', 'label_categoricals', 'hypothesis', 'task', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2324\n",
       "    })\n",
       "    train_hate: Dataset({\n",
       "        features: ['premise', 'label_categoricals', 'hypothesis', 'task', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    val_hate: Dataset({\n",
       "        features: ['premise', 'label_categoricals', 'hypothesis', 'task', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_hate: Dataset({\n",
       "        features: ['premise', 'label_categoricals', 'hypothesis', 'task', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2970\n",
       "    })\n",
       "    train_offensive: Dataset({\n",
       "        features: ['premise', 'label_categoricals', 'hypothesis', 'task', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11916\n",
       "    })\n",
       "    val_offensive: Dataset({\n",
       "        features: ['premise', 'label_categoricals', 'hypothesis', 'task', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1324\n",
       "    })\n",
       "    test_offensive: Dataset({\n",
       "        features: ['premise', 'label_categoricals', 'hypothesis', 'task', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 860\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_tweeteval, indice2logits\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "hypothesis_collection = {\n",
    "    \"hate\": \"This sentence contains offensive language toward women or immigrants.\",\n",
    "    \"offensive\": \"This sentence contains profanity or a targeted offense.\",\n",
    "}\n",
    "\n",
    "dataset_collection = {\n",
    "    task_name: (\n",
    "        dataset_dict.rename_columns({\"text\": \"premise\"}).map(\n",
    "            lambda rec: {\n",
    "                \"hypothesis\": len(rec[\"premise\"]) * [hypothesis_collection[task_name]],\n",
    "                \"task\": len(rec[\"premise\"]) * [task_name],\n",
    "            },\n",
    "            batched=True,\n",
    "            batch_size=None,\n",
    "        )\n",
    "    )\n",
    "    for task_name, dataset_dict in load_tweeteval(tasks=[\"hate\", \"offensive\"]).items()\n",
    "}\n",
    "\n",
    "merged_datasete_dict = DatasetDict(\n",
    "    {\n",
    "        \"train_merged\": concatenate_datasets(\n",
    "            [dataset_dict[\"train\"] for dataset_dict in dataset_collection.values()]\n",
    "        ),\n",
    "        \"val_merged\": concatenate_datasets(\n",
    "            [dataset_dict[\"val\"] for dataset_dict in dataset_collection.values()]\n",
    "        ),\n",
    "        **{\n",
    "            split + \"_\" + task_name: dataset_dict[split]\n",
    "            for task_name, dataset_dict in dataset_collection.items() for split in [\"train\", \"val\", \"test\"]\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "preprocessed_merged_dataset_dict = (\n",
    "    merged_datasete_dict\n",
    "    .map(\n",
    "        lambda rec: {\"labels\": (np.asarray(rec[\"labels\"]) * (-2) + 2)},\n",
    "        batched=True,\n",
    "        batch_size=None,\n",
    "    )\n",
    "    .map(lambda rec: (indice2logits(rec[\"labels\"], 3)), batched=True, batch_size=None)\n",
    "    .rename_columns({\"labels\": \"label_categoricals\", \"label_logits\": \"labels\"})\n",
    "    .map(\n",
    "        lambda rec: electra_tokenizer(\n",
    "            rec[\"premise\"],\n",
    "            rec[\"hypothesis\"],\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=8,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        ),\n",
    "        batched=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "preprocessed_merged_dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_metrics, f1_macro, get_labels, trainer_compute_metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"outputs/inference\",\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    overwrite_output_dir=True,\n",
    "    dataloader_num_workers=4,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=True,\n",
    "    eval_accumulation_steps=128,\n",
    "    optim=\"adamw_torch\",\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    hub_strategy=\"all_checkpoints\",\n",
    "    hub_token=secrets[\"hub_token_write\"],\n",
    ")\n",
    "\n",
    "\n",
    "def get_results(trainer, preprocessed_merged_dataset_dict):\n",
    "    return {\n",
    "        task: get_metrics(\n",
    "            lambda inputs: trainer.predict(inputs).predictions.argmax(axis=1),\n",
    "            preprocessed_merged_dataset_dict,\n",
    "            get_labels(preprocessed_merged_dataset_dict, \"label_categoricals\"),\n",
    "            [\"test\" + \"_\" + task],\n",
    "            {\"accuracy\": accuracy_score, \"f1\": f1_macro},\n",
    "        )\n",
    "        for task in [\"hate\", \"offensive\"]\n",
    "    }\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/chris-zeng/csci544-project/outputs/electra-nli-efl-tweeteval is already a clone of https://huggingface.co/ChrisZeng/electra-large-discriminator-nli-efl-tweeteval. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "Using amp half precision backend\n",
      "Loading model from outputs/electra-nli-efl-tweeteval/checkpoint-1793).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running training *****\n",
      "  Num examples = 20916\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 1630\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 11\n",
      "  Continuing training from global step 1793\n",
      "  Will skip the first 11 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fa2a3f4a56436498730416f5f306a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/electra-nli-efl-tweeteval/checkpoint-978 (score: 0.2981628179550171).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1793' max='1630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1793/1630 : < :, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/electra-nli-efl-tweeteval\n",
      "Configuration saved in outputs/electra-nli-efl-tweeteval/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl-tweeteval/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl-tweeteval/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl-tweeteval/special_tokens_map.json\n",
      "Saving model checkpoint to outputs/electra-nli-efl-tweeteval\n",
      "Configuration saved in outputs/electra-nli-efl-tweeteval/config.json\n",
      "Model weights saved in outputs/electra-nli-efl-tweeteval/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl-tweeteval/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl-tweeteval/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63ee16241c440a1b71119dec0db8e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 3.05k/3.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/ChrisZeng/electra-large-discriminator-nli-efl-tweeteval\n",
      "   867f048..3da15e3  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7943201376936316}, {'name': 'F1', 'type': 'f1', 'value': 0.7872381827932275}]}\n",
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 860\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hate': [('test_hate', 'accuracy', 0.5067340067340067),\n",
       "  ('test_hate', 'f1', 0.4490513878085375)],\n",
       " 'offensive': [('test_offensive', 'accuracy', 0.8453488372093023),\n",
       "  ('test_offensive', 'f1', 0.7984049350077109)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 10\n",
    "training_args.output_dir = \"outputs/electra-nli-efl-tweeteval\"\n",
    "training_args.push_to_hub = True\n",
    "training_args.hub_model_id = \"ChrisZeng/electra-large-discriminator-nli-efl-tweeteval\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
    "        num_labels=3,\n",
    "    ),\n",
    "    tokenizer=electra_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_merged_dataset_dict[\"train_merged\"],\n",
    "    eval_dataset=preprocessed_merged_dataset_dict[\"val_merged\"],\n",
    "    compute_metrics=trainer_compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model()\n",
    "results[\"Pretrained, no fine-tuning\"] = get_results(trainer, preprocessed_merged_dataset_dict)\n",
    "display(results[\"Pretrained, no fine-tuning\"])\n",
    "\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ChrisZeng/electra-large-discriminator-nli-efl-tweeteval/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/99b10439daab4c190f0e3d459fcca1a08206ecd1025629c45ff09b6fc9418f80.881c3c29416d595d47131c283e379a569a412b3cd806f88dedddedb9ab8e016d\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"ChrisZeng/electra-large-discriminator-nli-efl-tweeteval\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 1024,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ChrisZeng/electra-large-discriminator-nli-efl-tweeteval/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/65b5d7891706736fc778fa13ba96ec315998be7c7a2e579b82823a2dd659fff2.120f17681e492f990a83a5540ac0ac7e58c407187d4046c76984daaf96332941\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ChrisZeng/electra-large-discriminator-nli-efl-tweeteval.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "Using amp half precision backend\n",
      "Loading model from outputs/electra-nli-efl-tweeteval-hateval/checkpoint-700).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running training *****\n",
      "  Num examples = 9000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 700\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 10\n",
      "  Continuing training from global step 700\n",
      "  Will skip the first 10 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf76e9efd6c4eaea4e0c79457da6ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/electra-nli-efl-tweeteval-hateval/checkpoint-70 (score: 0.29623547196388245).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 : < :, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/electra-nli-efl-tweeteval-hateval\n",
      "Configuration saved in outputs/electra-nli-efl-tweeteval-hateval/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl-tweeteval-hateval/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl-tweeteval-hateval/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl-tweeteval-hateval/special_tokens_map.json\n",
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 860\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hate': [('test_hate', 'accuracy', 0.530976430976431),\n",
       "  ('test_hate', 'f1', 0.487723762777003)],\n",
       " 'offensive': [('test_offensive', 'accuracy', 0.8465116279069768),\n",
       "  ('test_offensive', 'f1', 0.7953488372093023)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 10\n",
    "training_args.output_dir = \"outputs/electra-nli-efl-tweeteval-hateval\"\n",
    "training_args.push_to_hub = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ChrisZeng/electra-large-discriminator-nli-efl-tweeteval\",\n",
    "        num_labels=3,\n",
    "    ),\n",
    "    tokenizer=electra_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_merged_dataset_dict[\"train_hate\"],\n",
    "    eval_dataset=preprocessed_merged_dataset_dict[\"val_hate\"],\n",
    "    compute_metrics=trainer_compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model()\n",
    "results[\"Pretrained, fine-tuned on HatEval\"] = get_results(trainer, preprocessed_merged_dataset_dict)\n",
    "display(results[\"Pretrained, fine-tuned on HatEval\"])\n",
    "\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ChrisZeng/electra-large-discriminator-nli-efl-tweeteval/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/99b10439daab4c190f0e3d459fcca1a08206ecd1025629c45ff09b6fc9418f80.881c3c29416d595d47131c283e379a569a412b3cd806f88dedddedb9ab8e016d\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"ChrisZeng/electra-large-discriminator-nli-efl-tweeteval\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 1024,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ChrisZeng/electra-large-discriminator-nli-efl-tweeteval/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/65b5d7891706736fc778fa13ba96ec315998be7c7a2e579b82823a2dd659fff2.120f17681e492f990a83a5540ac0ac7e58c407187d4046c76984daaf96332941\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ChrisZeng/electra-large-discriminator-nli-efl-tweeteval.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "Using amp half precision backend\n",
      "Loading model from outputs/electra-nli-efl-tweeteval-offenseval/checkpoint-930).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running training *****\n",
      "  Num examples = 11916\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 930\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 10\n",
      "  Continuing training from global step 930\n",
      "  Will skip the first 10 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3722bd6027419f8c438c2a5a16286b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/electra-nli-efl-tweeteval-offenseval/checkpoint-465 (score: 0.29025763273239136).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [930/930 : < :, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/electra-nli-efl-tweeteval-offenseval\n",
      "Configuration saved in outputs/electra-nli-efl-tweeteval-offenseval/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl-tweeteval-offenseval/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl-tweeteval-offenseval/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl-tweeteval-offenseval/special_tokens_map.json\n",
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 860\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hate': [('test_hate', 'accuracy', 0.5242424242424243),\n",
       "  ('test_hate', 'f1', 0.47969925632356814)],\n",
       " 'offensive': [('test_offensive', 'accuracy', 0.8546511627906976),\n",
       "  ('test_offensive', 'f1', 0.8137531596993022)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 10\n",
    "training_args.output_dir = \"outputs/electra-nli-efl-tweeteval-offenseval\"\n",
    "training_args.push_to_hub = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ChrisZeng/electra-large-discriminator-nli-efl-tweeteval\",\n",
    "        num_labels=3,\n",
    "    ),\n",
    "    tokenizer=electra_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_merged_dataset_dict[\"train_offensive\"],\n",
    "    eval_dataset=preprocessed_merged_dataset_dict[\"val_offensive\"],\n",
    "    compute_metrics=trainer_compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model()\n",
    "results[\"Pretrained, fine-tuned on OffensEval\"] = get_results(trainer, preprocessed_merged_dataset_dict)\n",
    "display(results[\"Pretrained, fine-tuned on OffensEval\"])\n",
    "\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/767fab951e9d8c432dc3775f2943a5208b7e3f6975863a23aaeba306a1c5980e.3104f0cd2cbab9afd68c2c65670667c1b6c00aa3da4c65b894d38f853ed1eb71\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 1024,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/9344f4058039030e7fa9899bee90c1134baefbee0c353c07f7e87d552e167f94.e6425238b58c11d46d8047b03a8fa1d7d4793e5cdbfb1724105c206e96f62291\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "Using amp half precision backend\n",
      "Loading model from outputs/electra-nli-efl-hate/checkpoint-700).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running training *****\n",
      "  Num examples = 9000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 700\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 10\n",
      "  Continuing training from global step 700\n",
      "  Will skip the first 10 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b74128f89a94483b3ffe2fe3e1dc777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/electra-nli-efl-hate/checkpoint-630 (score: 0.32740843296051025).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 : < :, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/electra-nli-efl-hate\n",
      "Configuration saved in outputs/electra-nli-efl-hate/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl-hate/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl-hate/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl-hate/special_tokens_map.json\n",
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 860\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hate': [('test_hate', 'accuracy', 0.5434343434343434),\n",
       "  ('test_hate', 'f1', 0.5049583814568481)],\n",
       " 'offensive': [('test_offensive', 'accuracy', 0.45232558139534884),\n",
       "  ('test_offensive', 'f1', 0.44742367050047815)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 10\n",
    "training_args.output_dir = \"outputs/electra-nli-efl-hate\"\n",
    "training_args.push_to_hub = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
    "        num_labels=3,\n",
    "    ),\n",
    "    tokenizer=electra_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_merged_dataset_dict[\"train_hate\"],\n",
    "    eval_dataset=preprocessed_merged_dataset_dict[\"val_hate\"],\n",
    "    compute_metrics=trainer_compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model()\n",
    "results[\"No pretraining, fine-tuned on HatEval\"] = get_results(trainer, preprocessed_merged_dataset_dict)\n",
    "display(results[\"No pretraining, fine-tuned on HatEval\"])\n",
    "\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/767fab951e9d8c432dc3775f2943a5208b7e3f6975863a23aaeba306a1c5980e.3104f0cd2cbab9afd68c2c65670667c1b6c00aa3da4c65b894d38f853ed1eb71\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 1024,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/9344f4058039030e7fa9899bee90c1134baefbee0c353c07f7e87d552e167f94.e6425238b58c11d46d8047b03a8fa1d7d4793e5cdbfb1724105c206e96f62291\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "Using amp half precision backend\n",
      "Loading model from outputs/electra-nli-efl-offensive/checkpoint-930).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running training *****\n",
      "  Num examples = 11916\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 930\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 10\n",
      "  Continuing training from global step 930\n",
      "  Will skip the first 10 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c430c7688a64caf839f53f2300073df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/electra-nli-efl-offensive/checkpoint-837 (score: 0.3044883608818054).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [930/930 : < :, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/electra-nli-efl-offensive\n",
      "Configuration saved in outputs/electra-nli-efl-offensive/config.json\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Model weights saved in outputs/electra-nli-efl-offensive/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/electra-nli-efl-offensive/tokenizer_config.json\n",
      "Special tokens file saved in outputs/electra-nli-efl-offensive/special_tokens_map.json\n",
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 860\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hate': [('test_hate', 'accuracy', 0.5579124579124579),\n",
       "  ('test_hate', 'f1', 0.5554955984533803)],\n",
       " 'offensive': [('test_offensive', 'accuracy', 0.85),\n",
       "  ('test_offensive', 'f1', 0.8061570557678497)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 10\n",
    "training_args.output_dir = \"outputs/electra-nli-efl-offensive\"\n",
    "training_args.push_to_hub = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
    "        num_labels=3,\n",
    "    ),\n",
    "    tokenizer=electra_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_merged_dataset_dict[\"train_offensive\"],\n",
    "    eval_dataset=preprocessed_merged_dataset_dict[\"val_offensive\"],\n",
    "    compute_metrics=trainer_compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model()\n",
    "results[\"No pretraining, fine-tuned on OffensEval\"] = get_results(trainer, preprocessed_merged_dataset_dict)\n",
    "display(results[\"No pretraining, fine-tuned on OffensEval\"])\n",
    "\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/767fab951e9d8c432dc3775f2943a5208b7e3f6975863a23aaeba306a1c5980e.3104f0cd2cbab9afd68c2c65670667c1b6c00aa3da4c65b894d38f853ed1eb71\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 1024,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/9344f4058039030e7fa9899bee90c1134baefbee0c353c07f7e87d552e167f94.e6425238b58c11d46d8047b03a8fa1d7d4793e5cdbfb1724105c206e96f62291\n",
      "/home/chris-zeng/csci544-project/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "Using amp half precision backend\n",
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2970\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: premise, task, hypothesis, label_categoricals.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 860\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hate': [('test_hate', 'accuracy', 0.2851851851851852),\n",
       "  ('test_hate', 'f1', 0.23873623528463087)],\n",
       " 'offensive': [('test_offensive', 'accuracy', 0.3058139534883721),\n",
       "  ('test_offensive', 'f1', 0.17580557496700053)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 10\n",
    "training_args.output_dir = \"outputs/electra-nli-efl\"\n",
    "training_args.push_to_hub = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
    "        num_labels=3,\n",
    "    ),\n",
    "    tokenizer=electra_tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=trainer_compute_metrics,\n",
    ")\n",
    "\n",
    "results[\"No pretraining, no fine-tuning\"] = get_results(trainer, preprocessed_merged_dataset_dict)\n",
    "display(results[\"No pretraining, no fine-tuning\"])\n",
    "\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Pretrained, no fine-tuning</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">hate</th>\n",
       "      <th>0</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.506734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.449051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">offensive</th>\n",
       "      <th>0</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.845349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.798405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Pretrained, fine-tuned on HatEval</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">hate</th>\n",
       "      <th>0</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.530976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.487724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">offensive</th>\n",
       "      <th>0</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.846512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.795349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Pretrained, fine-tuned on OffensEval</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">hate</th>\n",
       "      <th>0</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.524242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.479699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">offensive</th>\n",
       "      <th>0</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.854651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.813753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">No pretraining, fine-tuned on HatEval</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">hate</th>\n",
       "      <th>0</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.543434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.504958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">offensive</th>\n",
       "      <th>0</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.452326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.447424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">No pretraining, fine-tuned on OffensEval</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">hate</th>\n",
       "      <th>0</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.557912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.555496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">offensive</th>\n",
       "      <th>0</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.806157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">No pretraining, no fine-tuning</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">hate</th>\n",
       "      <th>0</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.285185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_hate</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.238736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">offensive</th>\n",
       "      <th>0</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.305814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_offensive</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.175806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   0  \\\n",
       "Pretrained, no fine-tuning               hate      0       test_hate   \n",
       "                                                   1       test_hate   \n",
       "                                         offensive 0  test_offensive   \n",
       "                                                   1  test_offensive   \n",
       "Pretrained, fine-tuned on HatEval        hate      0       test_hate   \n",
       "                                                   1       test_hate   \n",
       "                                         offensive 0  test_offensive   \n",
       "                                                   1  test_offensive   \n",
       "Pretrained, fine-tuned on OffensEval     hate      0       test_hate   \n",
       "                                                   1       test_hate   \n",
       "                                         offensive 0  test_offensive   \n",
       "                                                   1  test_offensive   \n",
       "No pretraining, fine-tuned on HatEval    hate      0       test_hate   \n",
       "                                                   1       test_hate   \n",
       "                                         offensive 0  test_offensive   \n",
       "                                                   1  test_offensive   \n",
       "No pretraining, fine-tuned on OffensEval hate      0       test_hate   \n",
       "                                                   1       test_hate   \n",
       "                                         offensive 0  test_offensive   \n",
       "                                                   1  test_offensive   \n",
       "No pretraining, no fine-tuning           hate      0       test_hate   \n",
       "                                                   1       test_hate   \n",
       "                                         offensive 0  test_offensive   \n",
       "                                                   1  test_offensive   \n",
       "\n",
       "                                                             1         2  \n",
       "Pretrained, no fine-tuning               hate      0  accuracy  0.506734  \n",
       "                                                   1        f1  0.449051  \n",
       "                                         offensive 0  accuracy  0.845349  \n",
       "                                                   1        f1  0.798405  \n",
       "Pretrained, fine-tuned on HatEval        hate      0  accuracy  0.530976  \n",
       "                                                   1        f1  0.487724  \n",
       "                                         offensive 0  accuracy  0.846512  \n",
       "                                                   1        f1  0.795349  \n",
       "Pretrained, fine-tuned on OffensEval     hate      0  accuracy  0.524242  \n",
       "                                                   1        f1  0.479699  \n",
       "                                         offensive 0  accuracy  0.854651  \n",
       "                                                   1        f1  0.813753  \n",
       "No pretraining, fine-tuned on HatEval    hate      0  accuracy  0.543434  \n",
       "                                                   1        f1  0.504958  \n",
       "                                         offensive 0  accuracy  0.452326  \n",
       "                                                   1        f1  0.447424  \n",
       "No pretraining, fine-tuned on OffensEval hate      0  accuracy  0.557912  \n",
       "                                                   1        f1  0.555496  \n",
       "                                         offensive 0  accuracy  0.850000  \n",
       "                                                   1        f1  0.806157  \n",
       "No pretraining, no fine-tuning           hate      0  accuracy  0.285185  \n",
       "                                                   1        f1  0.238736  \n",
       "                                         offensive 0  accuracy  0.305814  \n",
       "                                                   1        f1  0.175806  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.concat(\n",
    "    {\n",
    "        k: pd.concat({kk: pd.DataFrame(vv) for kk, vv in v.items()})\n",
    "        for k, v in results.items()\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
