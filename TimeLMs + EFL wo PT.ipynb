{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_token = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-2021-124m were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-2021-124m and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-2021-124m\", normalization=True\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-2021-124m\", num_labels=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__'],\n",
       "        num_rows: 27000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataframe = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"./data/hateval/hateval2019_en_train.csv\"),\n",
    "        pd.read_csv(\"./data/hateval/hateval2019_en_dev.csv\"),\n",
    "        pd.read_csv(\"./data/hateval/hateval2019_en_test.csv\"),\n",
    "    ],\n",
    "    keys=[\"train\", \"validation\", \"test\"],\n",
    "    names=[\"split\", \"index\"],\n",
    ").reset_index()\n",
    "\n",
    "hypotheses = pd.Series(\n",
    "    [\n",
    "        \"It is hate speech.\",\n",
    "        \"This sentence contains hate speech.\",\n",
    "        \"This sentence contains offensive language toward women or immigrants.\",\n",
    "    ],\n",
    "    name=\"hypothesis\",\n",
    ")\n",
    "\n",
    "dataframe = dataframe.merge(hypotheses, how=\"cross\").rename(columns={\"text\": \"premise\"})\n",
    "dataframe[\"label_categorical\"] = dataframe[\"HS\"] * (-2) + 2\n",
    "dataframe = dataframe[[\"split\", \"id\", \"premise\", \"hypothesis\", \"label_categorical\"]]\n",
    "\n",
    "datasets = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(dataframe[dataframe[\"split\"] == split])\n",
    "        for split in [\"train\", \"validation\", \"test\"]\n",
    "    }\n",
    ")\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1010e3b8858469f8b46edfd88bf8abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae0b953445945b4b3e6f78af1b02b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f35b06e4e134e0aa336d37fff37c00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad07f25a86d4b75822cc09a76dd48c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d17c7c1c0b4ddeb8876b77ea3cf776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce9bd66d1f145f0918bbee6a333bc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 27000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['split', 'id', 'premise', 'hypothesis', 'label_categorical', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def indice2logits(indice, num_classes):\n",
    "    indice = np.array(indice)\n",
    "    logits = np.zeros([len(indice), num_classes], dtype=float)\n",
    "    logits[np.arange(len(indice)), indice] = 1.0\n",
    "    return {\"label_logits\": logits}\n",
    "\n",
    "\n",
    "datasets = datasets.map(\n",
    "    lambda rec: tokenizer(\n",
    "        rec[\"premise\"],\n",
    "        rec[\"hypothesis\"],\n",
    "        padding=\"longest\",\n",
    "        max_length=512,\n",
    "        pad_to_multiple_of=8,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "datasets = datasets.map(\n",
    "    lambda rec: indice2logits(rec[\"label_categorical\"], 3),\n",
    "    batched=True,\n",
    "    batch_size=1024,\n",
    ")\n",
    "\n",
    "datasets = datasets.rename_column(\"label_logits\", \"labels\")\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, label_logits = eval_preds\n",
    "    preds = pred_logits.argmax(axis=1)\n",
    "    labels = label_logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/chris-zeng/csci544-project/outputs/time_lm_nli is already a clone of https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "Using amp half precision backend\n",
      "Loading model from outputs/time_lm_nli/checkpoint-6330).\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, __index_level_0__, premise, label_categorical, id, split. If hypothesis, __index_level_0__, premise, label_categorical, id, split are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 27000\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 6330\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 30\n",
      "  Continuing training from global step 6330\n",
      "  Will skip the first 30 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43a50397dce4591a3075ad39c7d724a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/time_lm_nli/checkpoint-4009 (score: 0.7930196752075032).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6330' max='6330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6330/6330 : < :, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/time_lm_nli\n",
      "Configuration saved in outputs/time_lm_nli/config.json\n",
      "Model weights saved in outputs/time_lm_nli/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/time_lm_nli/tokenizer_config.json\n",
      "Special tokens file saved in outputs/time_lm_nli/special_tokens_map.json\n",
      "Saving model checkpoint to outputs/time_lm_nli\n",
      "Configuration saved in outputs/time_lm_nli/config.json\n",
      "Model weights saved in outputs/time_lm_nli/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/time_lm_nli/tokenizer_config.json\n",
      "Special tokens file saved in outputs/time_lm_nli/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f54cfd719b4ddbb7679dabc8d081f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 3.05k/3.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval\n",
      "   f7b5b38..3351361  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7913333333333333}, {'name': 'F1', 'type': 'f1', 'value': 0.7899207605271177}]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/time_lm_nli\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=30,\n",
    "    logging_strategy=\"epoch\",\n",
    "    remove_unused_columns=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=128,\n",
    "    optim=\"adamw_apex_fused\",\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    learning_rate=1e-6,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"all_checkpoints\",\n",
    "    hub_model_id=\"ChrisZeng/twitter-roberta-base-efl-hateval\",\n",
    "    hub_token=hub_token,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_output = trainer.train(\n",
    "     resume_from_checkpoint=True,\n",
    ")\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /home/chris-zeng/.cache/huggingface/transformers/tmpxnkrb6lw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f80f71d63874e50a9939cc1c4e682f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/395 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/tokenizer_config.json in cache at /home/chris-zeng/.cache/huggingface/transformers/52fa2fc54deb3d2a20160be468621b71678a5472392303749a84c3f0cd6a20e6.a1b36501a87d1973ca00e86b765ffe71f426cd44cd9e8f063123840c161cfb5b\n",
      "creating metadata file for /home/chris-zeng/.cache/huggingface/transformers/52fa2fc54deb3d2a20160be468621b71678a5472392303749a84c3f0cd6a20e6.a1b36501a87d1973ca00e86b765ffe71f426cd44cd9e8f063123840c161cfb5b\n",
      "loading file https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/vocab.json from cache at /home/chris-zeng/.cache/huggingface/transformers/2cdccf28634cc183ab7a37d256612502177ae99e58f0031b1f0af8593fadfc89.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/merges.txt from cache at /home/chris-zeng/.cache/huggingface/transformers/af2b7cbb3983428236c5212830278a3e752a8ad2ff703e50b48929b45d571a71.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/tokenizer.json from cache at /home/chris-zeng/.cache/huggingface/transformers/a866ca950cf930da85964cd1a7a16279d9b4287bf0dad26721e889c50497707a.0c2ec527158cef7ca0a09741e615993b8753d06313787c12b61ae3d41c40e087\n",
      "loading file https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/special_tokens_map.json from cache at /home/chris-zeng/.cache/huggingface/transformers/31ef9c1ae2c373b9948b2f11b311addf38939914aed4b22aa405c65e9049b064.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/tokenizer_config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/52fa2fc54deb3d2a20160be468621b71678a5472392303749a84c3f0cd6a20e6.a1b36501a87d1973ca00e86b765ffe71f426cd44cd9e8f063123840c161cfb5b\n",
      "https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/chris-zeng/.cache/huggingface/transformers/tmpkf05qhxj\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001dbb0f83754e85a2aa3abdae050cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/946 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/config.json in cache at /home/chris-zeng/.cache/huggingface/transformers/8da03c3508aa6d4e4b598d7283c5aa609f8c3cb258cbb0bb4b366e06f6651e30.238fee94be8de4ee8a50af15fad79738c5b7b1e418024f07d61a75cdf765e3fb\n",
      "creating metadata file for /home/chris-zeng/.cache/huggingface/transformers/8da03c3508aa6d4e4b598d7283c5aa609f8c3cb258cbb0bb4b366e06f6651e30.238fee94be8de4ee8a50af15fad79738c5b7b1e418024f07d61a75cdf765e3fb\n",
      "loading configuration file https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/8da03c3508aa6d4e4b598d7283c5aa609f8c3cb258cbb0bb4b366e06f6651e30.238fee94be8de4ee8a50af15fad79738c5b7b1e418024f07d61a75cdf765e3fb\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ChrisZeng/twitter-roberta-base-efl-hateval\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ChrisZeng/twitter-roberta-base-efl-hateval/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/67ee4a10ecf9f4b4907e1d3a695f5b35e0b2158603feb91f657e9d053fdf5a05.81bc16d862c9752cf8524fd8397df0905d2d847d54566df93ecf55f44bd9990f\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ChrisZeng/twitter-roberta-base-efl-hateval.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, __index_level_0__, premise, label_categorical, id, split. If hypothesis, __index_level_0__, premise, label_categorical, id, split are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9000\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: hypothesis, __index_level_0__, premise, label_categorical, id, split. If hypothesis, __index_level_0__, premise, label_categorical, id, split are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>0.793</td>\n",
       "      <td>0.791765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.539</td>\n",
       "      <td>0.500086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy        f1\n",
       "dev      0.793  0.791765\n",
       "test     0.539  0.500086"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ChrisZeng/twitter-roberta-base-efl-hateval\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ChrisZeng/twitter-roberta-base-efl-hateval\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/inference\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=True,\n",
    "    eval_accumulation_steps=128,\n",
    "    disable_tqdm=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args)\n",
    "\n",
    "\n",
    "def predict(trainer, dataset):\n",
    "    preds = trainer.predict(dataset).predictions.argmax(axis=1)\n",
    "    df = (\n",
    "        pd.DataFrame(\n",
    "            {\"id\": dataset[\"id\"], \"pred\": preds, \"label\": dataset[\"label_categorical\"]}\n",
    "        )\n",
    "        .groupby(\"id\")\n",
    "        .mean()\n",
    "    )\n",
    "    df[\"pred\"] = (df[\"pred\"] > 1).astype(int) * 2\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "preds_test = predict(trainer, datasets[\"test\"])\n",
    "preds_dev = predict(trainer, datasets[\"validation\"])\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"dev\": {\n",
    "            \"accuracy\": accuracy_score(preds_dev[\"label\"], preds_dev[\"pred\"]),\n",
    "            \"f1\": f1_score(preds_dev[\"label\"], preds_dev[\"pred\"], average=\"macro\"),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_score(preds_test[\"label\"], preds_test[\"pred\"]),\n",
    "            \"f1\": f1_score(preds_test[\"label\"], preds_test[\"pred\"], average=\"macro\"),\n",
    "        },\n",
    "    }\n",
    ").transpose()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
