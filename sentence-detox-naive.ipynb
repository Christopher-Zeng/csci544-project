{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "pd.read_csv(\n",
    "    StringIO(\n",
    "        requests.get(\n",
    "            \"https://www.cs.cmu.edu/~biglou/resources/bad-words.txt\"\n",
    "        ).content.decode(\"utf-8\")\n",
    "    ),\n",
    "    names=[\"word\"],\n",
    ").to_csv(\"outputs/bad-words.csv\", index=False)\n",
    "\n",
    "pd.concat(\n",
    "    [\n",
    "        pd.read_html(\n",
    "            requests.get(f\"https://hatebase.org/search_results/page={page_num}\").content\n",
    "        )[0]\n",
    "        for page_num in range(1, 79)\n",
    "    ]\n",
    ").set_axis([\"word\", \"language\", \"sighting\", \"offensive\"], axis=1, inplace=False).to_csv(\n",
    "    \"outputs/hatebase.csv\", index=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class SentenceDetoxer(object):\n",
    "    def __init__(self, toxic_words):\n",
    "        self.replacements = {toxic_word: \"<CSD>\" for toxic_word in toxic_words}\n",
    "        self.pattern = \"|\".join(r\"\\b%s\\b\" % re.escape(s) for s in self.replacements)\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        return re.sub(\n",
    "            self.pattern, lambda match: self.replacements[match.group(0)], sentence\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    val: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 8679\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original', 'censored'],\n",
       "        num_rows: 3720\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = \"./data/toxic_spans\"\n",
    "filename = \"toxic_span_text_pairs.csv\"\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(data_path + \"/\" + filename))\n",
    "\n",
    "dataset_dict = dataset.train_test_split(test_size=2 / 10, seed=42)\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"val\": dataset_dict[\"test\"],\n",
    "        **dataset_dict[\"train\"].train_test_split(test_size=3 / 10, seed=42),\n",
    "    }\n",
    ")\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c76cca5740346e79f637491af38c544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2252f9fa8004b0cbad21801c1fdf20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8679 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b4d47ab0be4f56b7d116c8515442eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3720 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    val: Dataset({\n",
       "        features: ['original', 'censored', 'naive-detox'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['original', 'censored', 'naive-detox'],\n",
       "        num_rows: 8679\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original', 'censored', 'naive-detox'],\n",
       "        num_rows: 3720\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hatebase = pd.read_csv(\"outputs/hatebase.csv\")\n",
    "hate_words = hatebase[hatebase[\"language\"] == \"English\"][\"word\"]\n",
    "hate_words = hate_words.str.split(\"(\").str[0].str.strip()\n",
    "toxic_words = pd.concat(\n",
    "    [hate_words, pd.read_csv(\"outputs/bad-words.csv\")[\"word\"],]\n",
    ").drop_duplicates()\n",
    "\n",
    "detoxer = SentenceDetoxer(toxic_words)\n",
    "\n",
    "dataset_dict = dataset_dict.map(\n",
    "    lambda record: {\"naive-detox\": detoxer(record[\"original\"])}\n",
    ")\n",
    "\n",
    "dataset_dict\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bfe89fc0ce62c77ec618165bdab88a97265df2cf55337d24860c2391601a2f4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
