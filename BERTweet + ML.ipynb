{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataframe = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"./data/hateval2019_en_train.csv\"),\n",
    "        pd.read_csv(\"./data/hateval2019_en_dev.csv\"),\n",
    "        pd.read_csv(\"./data/hateval2019_en_test.csv\"),\n",
    "    ],\n",
    "    keys=[\"train\", \"dev\", \"test\"],\n",
    "    names=[\"split\", \"index\"],\n",
    ")\n",
    "\n",
    "datasets = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(dataframe.loc[(split)])\n",
    "        for split in [\"train\", \"dev\", \"test\"]\n",
    "    }\n",
    ")\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/b0d7660a1cf1cc386b57bf9307b4bc6f23b17e384049be92e2068b42dd6faafc.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-covid19-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/vocab.txt from cache at /home/chris-zeng/.cache/huggingface/transformers/3c6acc8a22458fe4a27ae1add9051cbf7ff985db5787231befd21bf5ee34b043.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/bpe.codes from cache at /home/chris-zeng/.cache/huggingface/transformers/8651b0811f907869b9db76ec3ee744f2fc7692a8ab25b2c96e1a24bcc6b95015.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/b0d7660a1cf1cc386b57bf9307b4bc6f23b17e384049be92e2068b42dd6faafc.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-covid19-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/b0d7660a1cf1cc386b57bf9307b4bc6f23b17e384049be92e2068b42dd6faafc.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-covid19-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/ab0809f67d77051488b63da3a853d82293dbc8358d01aef03b3390b54d612da7.9e82cf9c093a5e60e05371a1c0531b65a8b3f040d46d1369c15d874b8cbdf8fa\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-cased and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"vinai/bertweet-covid19-base-cased\", normalization=True\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-covid19-base-cased\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels_logits = eval_preds\n",
    "    preds = pred_logits.argmax(axis=1)\n",
    "    labels = labels_logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5950cba3cf5e44c481d9e9332d709845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdde9461a09d477e972412b4f2b81e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88edc5e215f4ff7ada7cdb539c3a58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0e3722908d498296377be739dbad49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261c971fe5824b40a2e2d35270be0dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5296de5e64045f78fc4905f44d8a487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def indice2logits(indice, num_classes):\n",
    "    indice = np.array(indice)\n",
    "    logits = np.zeros([len(indice), num_classes], dtype=float)\n",
    "    logits[np.arange(len(indice)), indice] = 1.0\n",
    "    return {\"label_logits\": logits}\n",
    "\n",
    "\n",
    "datasets = datasets.map(\n",
    "    lambda rec: tokenizer(\n",
    "        rec[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=192,\n",
    "        pad_to_multiple_of=8,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "    ),\n",
    "    batched=True,\n",
    "    keep_in_memory=True,\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "datasets = datasets.map(\n",
    "    lambda rec: indice2logits(rec[\"HS\"], 2),\n",
    "    batched=True,\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "\n",
    "datasets = datasets.rename_column(\"label_logits\", \"labels\")\n",
    "datasets = datasets.remove_columns([])\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bertweet\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=50,\n",
    "    logging_strategy=\"epoch\",\n",
    "    remove_unused_columns=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=128,\n",
    "    optim=\"adamw_apex_fused\",\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    learning_rate=1e-6,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"dev\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 9000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 3500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 1:18:15, Epoch 49/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.678662</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.364272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.666452</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.364272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.641345</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.584775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.631200</td>\n",
       "      <td>0.602661</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.700743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.589100</td>\n",
       "      <td>0.565582</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.734705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.551200</td>\n",
       "      <td>0.540997</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.746294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.525015</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.752988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.511515</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.755474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.502811</td>\n",
       "      <td>0.769000</td>\n",
       "      <td>0.768398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.492053</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.779308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.484857</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.779357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.476720</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.785278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.475011</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.785506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.469167</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.791346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.465712</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.792986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.464869</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.791145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.394900</td>\n",
       "      <td>0.463244</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.791891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.390100</td>\n",
       "      <td>0.463021</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.795161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.382200</td>\n",
       "      <td>0.467197</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.788645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.466001</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.792617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.461865</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.800103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.462528</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.800257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.460742</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.799992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.463327</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.803244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.359900</td>\n",
       "      <td>0.467691</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.793441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.466586</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.797318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.465843</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.801186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.344700</td>\n",
       "      <td>0.466396</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.799228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.465960</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.799015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.474284</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.793524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>0.469888</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.796338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.473020</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.794234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.468165</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.799874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.473872</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.797365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.470720</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.800103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.475167</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.798390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.321500</td>\n",
       "      <td>0.470660</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.798094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.471672</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.797116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.473512</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.796191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.473156</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.794076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.475756</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.795263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.473506</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.799071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.478609</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.795311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.475734</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.795161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>0.474589</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.798038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.475816</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.798094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.476254</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.798094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.476472</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.797116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.475959</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.800048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.309100</td>\n",
       "      <td>0.476352</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.798094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-70\n",
      "Configuration saved in outputs/bertweet/checkpoint-70/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-70/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-140\n",
      "Configuration saved in outputs/bertweet/checkpoint-140/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-140/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-210\n",
      "Configuration saved in outputs/bertweet/checkpoint-210/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-210/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-280\n",
      "Configuration saved in outputs/bertweet/checkpoint-280/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-280/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-350\n",
      "Configuration saved in outputs/bertweet/checkpoint-350/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-350/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-420\n",
      "Configuration saved in outputs/bertweet/checkpoint-420/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-420/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-490\n",
      "Configuration saved in outputs/bertweet/checkpoint-490/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-490/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-560\n",
      "Configuration saved in outputs/bertweet/checkpoint-560/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-560/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-630\n",
      "Configuration saved in outputs/bertweet/checkpoint-630/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-630/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-700\n",
      "Configuration saved in outputs/bertweet/checkpoint-700/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-700/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-770\n",
      "Configuration saved in outputs/bertweet/checkpoint-770/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-770/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-840\n",
      "Configuration saved in outputs/bertweet/checkpoint-840/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-840/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-910\n",
      "Configuration saved in outputs/bertweet/checkpoint-910/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-910/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-980\n",
      "Configuration saved in outputs/bertweet/checkpoint-980/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-980/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1050\n",
      "Configuration saved in outputs/bertweet/checkpoint-1050/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1050/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1120\n",
      "Configuration saved in outputs/bertweet/checkpoint-1120/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1120/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1190\n",
      "Configuration saved in outputs/bertweet/checkpoint-1190/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1190/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1260\n",
      "Configuration saved in outputs/bertweet/checkpoint-1260/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1260/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1330\n",
      "Configuration saved in outputs/bertweet/checkpoint-1330/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1330/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1400\n",
      "Configuration saved in outputs/bertweet/checkpoint-1400/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1470\n",
      "Configuration saved in outputs/bertweet/checkpoint-1470/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1470/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1540\n",
      "Configuration saved in outputs/bertweet/checkpoint-1540/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1540/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1610\n",
      "Configuration saved in outputs/bertweet/checkpoint-1610/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1610/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1680\n",
      "Configuration saved in outputs/bertweet/checkpoint-1680/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1680/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1750\n",
      "Configuration saved in outputs/bertweet/checkpoint-1750/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1750/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1820\n",
      "Configuration saved in outputs/bertweet/checkpoint-1820/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1820/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1890\n",
      "Configuration saved in outputs/bertweet/checkpoint-1890/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1890/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-1960\n",
      "Configuration saved in outputs/bertweet/checkpoint-1960/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-1960/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2030\n",
      "Configuration saved in outputs/bertweet/checkpoint-2030/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2030/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2100\n",
      "Configuration saved in outputs/bertweet/checkpoint-2100/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2100/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2170\n",
      "Configuration saved in outputs/bertweet/checkpoint-2170/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2170/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2240\n",
      "Configuration saved in outputs/bertweet/checkpoint-2240/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2240/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2310\n",
      "Configuration saved in outputs/bertweet/checkpoint-2310/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2310/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2380\n",
      "Configuration saved in outputs/bertweet/checkpoint-2380/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2380/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2450\n",
      "Configuration saved in outputs/bertweet/checkpoint-2450/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2450/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2520\n",
      "Configuration saved in outputs/bertweet/checkpoint-2520/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2520/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2590\n",
      "Configuration saved in outputs/bertweet/checkpoint-2590/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2590/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2660\n",
      "Configuration saved in outputs/bertweet/checkpoint-2660/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2660/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2730\n",
      "Configuration saved in outputs/bertweet/checkpoint-2730/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2730/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2800\n",
      "Configuration saved in outputs/bertweet/checkpoint-2800/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2870\n",
      "Configuration saved in outputs/bertweet/checkpoint-2870/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2870/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-2940\n",
      "Configuration saved in outputs/bertweet/checkpoint-2940/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-2940/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-3010\n",
      "Configuration saved in outputs/bertweet/checkpoint-3010/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-3010/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-3080\n",
      "Configuration saved in outputs/bertweet/checkpoint-3080/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-3080/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-3150\n",
      "Configuration saved in outputs/bertweet/checkpoint-3150/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-3150/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-3220\n",
      "Configuration saved in outputs/bertweet/checkpoint-3220/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-3220/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-3290\n",
      "Configuration saved in outputs/bertweet/checkpoint-3290/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-3290/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-3360\n",
      "Configuration saved in outputs/bertweet/checkpoint-3360/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-3360/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-3430\n",
      "Configuration saved in outputs/bertweet/checkpoint-3430/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-3430/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/bertweet/checkpoint-3500\n",
      "Configuration saved in outputs/bertweet/checkpoint-3500/config.json\n",
      "Model weights saved in outputs/bertweet/checkpoint-3500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/bertweet/checkpoint-1680 (score: 0.804).\n",
      "Saving model checkpoint to outputs/bertweet\n",
      "Configuration saved in outputs/bertweet/config.json\n",
      "Model weights saved in outputs/bertweet/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer_output = trainer.train()\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text, TR, AG, index, HS. If id, text, TR, AG, index, HS are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.544, 'f1': 0.5105746407064516}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(datasets[\"test\"]).predictions.argmax(axis = 1)\n",
    "labels = datasets[\"test\"][\"HS\"]\n",
    "\n",
    "{\n",
    "    'accuracy': accuracy_score(labels, preds),\n",
    "    'f1': f1_score(labels, preds, average='macro'),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
