{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_token = \"<HUB_TOKEN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataframe = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"./data/hateval2019_en_train.csv\"),\n",
    "        pd.read_csv(\"./data/hateval2019_en_dev.csv\"),\n",
    "        pd.read_csv(\"./data/hateval2019_en_test.csv\"),\n",
    "    ],\n",
    "    keys=[\"train\", \"dev\", \"test\"],\n",
    "    names=[\"split\", \"index\"],\n",
    ")\n",
    "\n",
    "datasets = DatasetDict(\n",
    "    {\n",
    "        split: Dataset.from_pandas(dataframe.loc[(split)])\n",
    "        for split in [\"train\", \"dev\", \"test\"]\n",
    "    }\n",
    ")\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/b0d7660a1cf1cc386b57bf9307b4bc6f23b17e384049be92e2068b42dd6faafc.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-covid19-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/vocab.txt from cache at /home/chris-zeng/.cache/huggingface/transformers/3c6acc8a22458fe4a27ae1add9051cbf7ff985db5787231befd21bf5ee34b043.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/bpe.codes from cache at /home/chris-zeng/.cache/huggingface/transformers/8651b0811f907869b9db76ec3ee744f2fc7692a8ab25b2c96e1a24bcc6b95015.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/b0d7660a1cf1cc386b57bf9307b4bc6f23b17e384049be92e2068b42dd6faafc.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-covid19-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/b0d7660a1cf1cc386b57bf9307b4bc6f23b17e384049be92e2068b42dd6faafc.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-covid19-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/vinai/bertweet-covid19-base-cased/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/ab0809f67d77051488b63da3a853d82293dbc8358d01aef03b3390b54d612da7.9e82cf9c093a5e60e05371a1c0531b65a8b3f040d46d1369c15d874b8cbdf8fa\n",
      "Some weights of the model checkpoint at vinai/bertweet-covid19-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-covid19-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"vinai/bertweet-covid19-base-cased\", normalization=True\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-covid19-base-cased\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels_logits = eval_preds\n",
    "    preds = pred_logits.argmax(axis=1)\n",
    "    labels = labels_logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f24871b004f4503941d1fdfcb4f0f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b5792676334a4aad58a91a3b861a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea665c59a4f46c3be47839902bb07c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea83d9b43e4948b581bd78443fe04371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bee11378d444b10b59271d164aada60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ff6f716cb749b1bf13f092e89dc8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'HS', 'TR', 'AG', 'index', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def indice2logits(indice, num_classes):\n",
    "    indice = np.array(indice)\n",
    "    logits = np.zeros([len(indice), num_classes], dtype=float)\n",
    "    logits[np.arange(len(indice)), indice] = 1.0\n",
    "    return {\"label_logits\": logits}\n",
    "\n",
    "\n",
    "datasets = datasets.map(\n",
    "    lambda rec: tokenizer(\n",
    "        rec[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=192,\n",
    "        pad_to_multiple_of=8,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "    ),\n",
    "    batched=True,\n",
    "    keep_in_memory=True,\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "datasets = datasets.map(\n",
    "    lambda rec: indice2logits(rec[\"HS\"], 2),\n",
    "    batched=True,\n",
    "    keep_in_memory=True,\n",
    ")\n",
    "\n",
    "datasets = datasets.rename_column(\"label_logits\", \"labels\")\n",
    "datasets = datasets.remove_columns([])\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bertweet\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=30,\n",
    "    logging_strategy=\"epoch\",\n",
    "    remove_unused_columns=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=128,\n",
    "    optim=\"adamw_apex_fused\",\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    learning_rate=1e-6,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"all_checkpoints\",\n",
    "    hub_model_id=\"ChrisZeng/bertweet-base-cased-covid19-hateval\",\n",
    "    hub_token=hub_token,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"dev\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_output = trainer.train(\n",
    "    resume_from_checkpoint=True,\n",
    ")\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/ChrisZeng/bertweet-base-cased-covid19-hateval/resolve/main/vocab.txt from cache at /home/chris-zeng/.cache/huggingface/transformers/a65a5273741a95af789ee6a68080517bc0814ee866519c1abf0635f8699f3826.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
      "loading file https://huggingface.co/ChrisZeng/bertweet-base-cased-covid19-hateval/resolve/main/bpe.codes from cache at /home/chris-zeng/.cache/huggingface/transformers/e27b892496ae2e66b946e87a21ef55b71bc2823ca0bc3d534ef5333f06f20f1e.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
      "loading file https://huggingface.co/ChrisZeng/bertweet-base-cased-covid19-hateval/resolve/main/added_tokens.json from cache at /home/chris-zeng/.cache/huggingface/transformers/7244bc040ffc27b11cbd505436c8c8fb6d30f66c64cb7e608bdea8e25283f249.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n",
      "loading file https://huggingface.co/ChrisZeng/bertweet-base-cased-covid19-hateval/resolve/main/special_tokens_map.json from cache at /home/chris-zeng/.cache/huggingface/transformers/2bc92625678483922ef4fdec44ea4ad7f692ce619315c0257d92df67f097d094.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n",
      "loading file https://huggingface.co/ChrisZeng/bertweet-base-cased-covid19-hateval/resolve/main/tokenizer_config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/a63b53197b4df5b6bb2d4081d9542ca2f843188e282044a1ee6f41f898288efe.7322343df5e47ca0bca14ef7e68b0df812a2d6b3b023263eebfe11f699ca0873\n",
      "Adding <mask> to the vocabulary\n",
      "loading configuration file https://huggingface.co/ChrisZeng/bertweet-base-cased-covid19-hateval/resolve/main/config.json from cache at /home/chris-zeng/.cache/huggingface/transformers/bcd2dc596613472d1a98ebb6e3bca0154bface5cea3486a1eba731c79807c775.83e32d48864548c5d9c0c7bb78a145370fc0692d0d2ef12a5c27e110c08f5a52\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"ChrisZeng/bertweet-base-cased-covid19-hateval\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ChrisZeng/bertweet-base-cased-covid19-hateval/resolve/main/pytorch_model.bin from cache at /home/chris-zeng/.cache/huggingface/transformers/156e501544981f8eb71c0a14f3cde23967beec7e34a1ca80ce7cb794523f22d4.88d1f4b6fb08aecf1bb538c90ff4989f44a5fa7eb3cfa80a64088a9f10621157\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ChrisZeng/bertweet-base-cased-covid19-hateval.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, index, HS, TR, id, AG. If text, index, HS, TR, id, AG are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, index, HS, TR, id, AG. If text, index, HS, TR, id, AG are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.771176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.553667</td>\n",
       "      <td>0.524164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy        f1\n",
       "dev   0.772000  0.771176\n",
       "test  0.553667  0.524164"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ChrisZeng/bertweet-base-cased-covid19-hateval\"\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ChrisZeng/bertweet-base-cased-covid19-hateval\"\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/inference\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=True,\n",
    "    eval_accumulation_steps=128,\n",
    "    disable_tqdm=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args)\n",
    "\n",
    "\n",
    "preds_test = trainer.predict(datasets[\"test\"]).predictions.argmax(axis=1)\n",
    "preds_dev = trainer.predict(datasets[\"dev\"]).predictions.argmax(axis=1)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"dev\": {\n",
    "            \"accuracy\": accuracy_score(datasets[\"dev\"][\"HS\"], preds_dev),\n",
    "            \"f1\": f1_score(datasets[\"dev\"][\"HS\"], preds_dev, average=\"macro\"),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"accuracy\": accuracy_score(datasets[\"test\"][\"HS\"], preds_test),\n",
    "            \"f1\": f1_score(datasets[\"test\"][\"HS\"], preds_test, average=\"macro\"),\n",
    "        },\n",
    "    }\n",
    ").transpose()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f0e8d4354f47dbf330babecd1ea115412090f176c68201edfbe45cb7bacd91"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
